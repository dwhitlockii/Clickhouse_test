2025.07.02 07:41:42.912798 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:41:42.918353 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:41:42.918768 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:41:42.919315 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:41:42.919868 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:41:42.922514 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:41:42.927329 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:41:42.927719 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:41:42.934220 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:41:42.935454 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:41:42.935823 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:41:42.936131 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:41:42.936395 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:41:42.936602 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:41:42.982121 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 48.66 KiB to 26.02 MiB
2025.07.02 07:41:43.066706 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:41:43.067693 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:41:43.083672 [ 1 ] {} <Information> Application: Background threads finished in 16 ms
2025.07.02 07:41:43.111479 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:41:43.115253 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:41:43.115537 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:41:44.119217 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:41:44.122585 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:41:44.122863 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:41:44.123467 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:41:44.123886 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:41:44.125010 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:41:44.128335 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:41:44.129124 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:41:44.143115 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:41:44.146560 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:41:44.146833 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:41:44.147049 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:41:44.147264 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:41:44.147487 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:41:44.181750 [ 27 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 48.66 KiB to 26.04 MiB
2025.07.02 07:41:44.208459 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:41:44.209068 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:41:44.209811 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:41:44.214114 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:41:44.214673 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:41:44.214950 [ 26 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:41:45.241757 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:41:45.242738 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:41:45.243197 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:41:45.244437 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:41:45.245101 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:41:45.246505 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:41:45.250075 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:41:45.250546 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:41:45.259769 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:41:45.260857 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:41:45.263284 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:41:45.266304 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:41:45.266588 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:41:45.266820 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:41:45.302876 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 48.66 KiB to 26.04 MiB
2025.07.02 07:41:45.314075 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:41:45.315537 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:41:45.316002 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:41:45.317814 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:41:45.319076 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:41:45.319692 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:41:46.083845 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:41:46.084367 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:41:46.084687 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:41:46.085190 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:41:46.085564 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:41:46.086897 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:41:46.092047 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:41:46.092308 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:41:46.097071 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:41:46.097503 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:41:46.097745 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:41:46.097937 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:41:46.098166 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:41:46.098381 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:41:46.144573 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 48.66 KiB to 26.04 MiB
2025.07.02 07:41:46.150377 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:41:46.151267 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:41:46.151834 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:41:46.152595 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:41:46.153068 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:41:46.153388 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:41:47.299594 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:41:47.300326 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:41:47.300633 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:41:47.301037 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:41:47.301455 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:41:47.302939 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:41:47.307832 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:41:47.308163 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:41:47.313030 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:41:47.313338 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:41:47.313541 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:41:47.313814 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:41:47.314406 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:41:47.314601 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:41:47.360554 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 48.66 KiB to 26.04 MiB
2025.07.02 07:41:47.366440 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:41:47.367459 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:41:47.368134 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:41:47.368662 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:41:47.369119 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:41:47.369610 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:41:49.484137 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:41:49.484665 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:41:49.485005 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:41:49.485604 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:41:49.486236 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:41:49.487782 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:41:49.492913 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:41:49.493316 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:41:49.497779 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:41:49.499308 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:41:49.499693 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:41:49.500199 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:41:49.500430 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:41:49.501152 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:41:49.546513 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 48.66 KiB to 26.04 MiB
2025.07.02 07:41:49.563759 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:41:49.564920 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:41:49.565714 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:41:49.566584 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:41:49.567187 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:41:49.567506 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:41:53.192858 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:41:53.193736 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:41:53.194027 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:41:53.194433 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:41:53.209860 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:41:53.211879 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:41:53.216253 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:41:53.216521 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:41:53.221572 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:41:53.221971 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:41:53.222219 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:41:53.222604 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:41:53.222874 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:41:53.223145 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:41:53.268696 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 48.66 KiB to 26.04 MiB
2025.07.02 07:41:53.287170 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:41:53.288371 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:41:53.288918 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:41:53.289409 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:41:53.290079 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:41:53.290475 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:42:00.075498 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:42:00.076013 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:42:00.076283 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:42:00.076816 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:42:00.077276 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:42:00.078440 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:42:00.082198 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:42:00.082502 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:42:00.086878 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:42:00.087283 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:42:00.087583 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:42:00.087875 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:42:00.088152 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:42:00.088409 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:42:00.133068 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:42:00.134711 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 5.97 KiB to 26.05 MiB
2025.07.02 07:42:00.135114 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:42:00.135569 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:42:00.136111 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:42:00.136691 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:42:00.137076 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:42:13.284950 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:42:13.285360 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:42:13.285665 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:42:13.286339 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:42:13.286664 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:42:13.287807 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:42:13.291008 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:42:13.291409 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:42:13.294907 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:42:13.295222 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:42:13.295599 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:42:13.295840 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:42:13.296129 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:42:13.296374 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:42:13.340247 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:42:13.341337 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:42:13.341796 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:42:13.342283 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:42:13.342776 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:42:13.343049 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:42:39.355785 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:42:39.356208 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:42:39.356506 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:42:39.357085 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:42:39.357437 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:42:39.358572 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:42:39.361502 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:42:39.361732 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:42:39.366836 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:42:39.367219 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:42:39.367424 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:42:39.367659 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:42:39.367851 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:42:39.368073 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:42:39.410749 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:42:39.411708 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:42:39.412172 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:42:39.414742 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:42:39.415206 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:42:39.415493 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:31.024414 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:31.024915 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:31.025174 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:31.025578 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:31.026004 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:31.027331 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:31.030492 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:31.030730 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:31.035649 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:31.036126 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:31.036407 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:31.036654 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:31.036873 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:31.037148 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:31.082331 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:31.083119 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 30.34 KiB to 26.48 MiB
2025.07.02 07:43:31.083660 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:31.084090 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:31.085345 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:31.085828 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:31.086160 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:37.906869 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:37.907296 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:37.907570 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:37.908134 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:37.908563 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:37.909783 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:37.913140 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:37.913502 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:37.917307 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:37.917773 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:37.918002 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:37.918208 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:37.918385 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:37.918629 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:37.966434 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 81.09 KiB to 26.66 MiB
2025.07.02 07:43:37.967879 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:37.968961 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:37.969541 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:37.970359 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:37.970776 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:37.971006 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:38.640301 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:38.640804 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:38.641171 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:38.641574 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:38.641981 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:38.643242 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:38.646644 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:38.646952 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:38.650976 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:38.651451 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:38.651742 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:38.651983 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:38.652253 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:38.652451 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:38.699278 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 81.09 KiB to 26.62 MiB
2025.07.02 07:43:38.702096 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:38.703203 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:38.703742 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:38.704289 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:38.704767 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:38.705068 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:39.316322 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:39.316836 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:39.317092 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:39.317483 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:39.317950 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:39.319235 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:39.322245 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:39.322541 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:39.326169 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:39.326528 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:39.326810 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:39.327079 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:39.327341 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:39.327551 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:39.370822 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:39.371764 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:39.372288 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:39.372770 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:39.373184 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:39.373474 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:40.119138 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:40.119538 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:40.119783 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:40.120282 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:40.120600 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:40.121738 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:40.124614 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:40.125014 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:40.128461 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:40.128972 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:40.129207 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:40.129428 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:40.129595 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:40.129814 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:40.173331 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:40.174289 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:40.174901 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:40.175410 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:40.175864 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:40.176210 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:41.323020 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:41.323471 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:41.323759 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:41.324401 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:41.324819 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:41.326158 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:41.329495 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:41.329855 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:41.334659 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:41.335083 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:41.335322 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:41.335504 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:41.335789 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:41.336043 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:41.381481 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:41.382404 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 5.97 KiB to 26.19 MiB
2025.07.02 07:43:41.382855 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:41.383357 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:41.384004 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:41.384434 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:41.384738 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:43.337373 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:43.337841 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:43.338058 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:43.338471 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:43.338980 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:43.340303 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:43.343518 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:43.343835 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:43.347087 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:43.347468 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:43.347718 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:43.347981 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:43.348213 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:43.348438 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:43.392616 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:43.393477 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:43.394075 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:43.394632 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:43.395102 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:43.395388 [ 29 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:46.948259 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:46.948753 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:46.948974 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:46.949423 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:46.949851 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:46.950921 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:46.954867 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:46.955268 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:46.958701 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:46.959058 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:46.959302 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:46.959585 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:46.959835 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:46.960094 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:47.008075 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 72.98 KiB to 26.34 MiB
2025.07.02 07:43:47.013261 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:47.014060 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:47.014559 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:47.015417 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:47.015972 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:47.016349 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:43:53.793083 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:43:53.793562 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:43:53.793902 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:43:53.794390 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:43:53.794905 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:43:53.796074 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:43:53.798954 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:43:53.799255 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:43:53.802568 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:43:53.802954 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:43:53.803218 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:43:53.803435 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:43:53.803632 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:43:53.803895 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:43:53.847182 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:43:53.848220 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:43:53.848630 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:43:53.849216 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:43:53.849713 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:43:53.850048 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:44:07.010664 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:44:07.011193 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:44:07.011431 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:44:07.011876 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:44:07.012314 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:44:07.013563 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:44:07.016957 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:44:07.017239 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:44:07.021302 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:44:07.021769 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:44:07.022032 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:44:07.022278 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:44:07.022558 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:44:07.022830 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:44:07.069332 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 81.09 KiB to 26.66 MiB
2025.07.02 07:44:07.071211 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:44:07.072076 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:44:07.072585 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:44:07.073106 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:44:07.073585 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:44:07.073946 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:44:33.053934 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:44:33.054475 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:44:33.054764 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:44:33.055371 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:44:33.055874 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:44:33.057206 [ 1 ] {} <Warning> Application: Effective user of the process (clickhouse) does not match the owner of the data (root).
2025.07.02 07:44:33.060421 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:44:33.060709 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:44:33.063730 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:44:33.064050 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:44:33.064302 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:44:33.064545 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:44:33.064755 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:44:33.064981 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:44:33.110914 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:44:33.113253 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 5.97 KiB to 26.05 MiB
2025.07.02 07:44:33.113599 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:44:33.114280 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:44:33.114932 [ 1 ] {} <Error> Application: Code: 568. DB::Exception: Our server id 1 not found in raft_configuration section. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x00000000060e74ac
2. DB::Exception::Exception<int const&>(int, FormatStringHelperImpl<std::type_identity<int const&>::type>, int const&) @ 0x0000000006a25d2b
3. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f4c2a
4. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
5. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
6. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
7. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
8. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
9. Poco::Util::Application::run() @ 0x0000000012bcfba6
10. DB::Keeper::run() @ 0x00000000060dc330
11. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
12. main @ 0x00000000060d9b73
13. ? @ 0x0000000000029d90
14. ? @ 0x0000000000029e40
15. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:44:33.115399 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:44:33.115696 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:45:24.729964 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:45:24.730830 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:45:24.731139 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:45:24.731701 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:45:24.732102 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:45:24.734177 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:45:24.734470 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:45:24.739978 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:45:24.740503 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:45:24.740738 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:45:24.740995 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:45:24.741242 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:45:24.741474 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:45:24.753406 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 07:45:24.754761 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 07:45:24.755883 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 07:45:24.756623 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 07:45:24.768517 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 07:45:24.768998 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 07:45:24.769261 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 07:45:24.769670 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 07:45:24.769947 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 07:45:24.770173 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 07:45:24.770808 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 07:45:24.770907 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 07:45:24.787152 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.80 MiB
2025.07.02 07:45:25.952610 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 07:45:25.953362 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 07:45:25.953864 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 07:45:25.956577 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 07:45:25.956999 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 07:45:25.957290 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 07:45:25.960211 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 07:45:25.960764 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 07:45:25.961076 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 07:45:25.961437 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 07:45:25.963901 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): [::1]:2181
2025.07.02 07:45:25.964198 [ 1 ] {} <Information> Application: Listening for Prometheus: http://[::1]:9363
2025.07.02 07:45:25.964528 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 127.0.0.1:2181
2025.07.02 07:45:25.964826 [ 1 ] {} <Information> Application: Listening for Prometheus: http://127.0.0.1:9363
2025.07.02 07:45:25.976169 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:45:25.977452 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:45:25.978014 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:45:25.978789 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 07:45:25.979027 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 07:45:25.979661 [ 69 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 07:45:28.812226 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 07:45:28.812708 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 07:45:28.813118 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 07:45:28.815502 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 07:45:28.815845 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 07:45:28.963373 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 07:45:28.963895 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 07:45:28.964170 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 07:45:28.964420 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 07:45:28.964717 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 07:45:28.964977 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 07:45:28.965177 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 07:45:28.965419 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 07:45:28.965421 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 07:45:28.965888 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 07:45:28.966121 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 07:45:28.966412 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 07:45:28.966715 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 07:45:28.966959 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 07:45:28.967142 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 07:45:28.967389 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 07:45:28.979033 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 07:45:28.979421 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 12059 us elapsed
2025.07.02 07:45:28.979861 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 07:45:28.980017 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 07:45:28.979907 [ 47 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 07:45:28.980410 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 07:45:28.980061 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 07:45:28.980162 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 07:45:28.980094 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 07:45:28.980230 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 07:45:28.980413 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 07:45:28.980239 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 07:45:28.980297 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 07:45:28.980347 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 07:45:28.979905 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 07:45:28.980511 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 07:45:28.980281 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 07:45:28.980549 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 07:45:28.980125 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 07:45:28.986708 [ 42 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 07:45:28.987532 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:45:28.989091 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 07:45:28.989828 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:45:28.990717 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:45:28.990997 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:45:28.991264 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:45:33.823403 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:45:33.823919 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:45:33.824253 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:45:33.824845 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:45:33.839148 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:45:33.847504 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:45:33.847824 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:45:33.855332 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:45:33.855659 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:45:33.855899 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:45:33.856328 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:45:33.858657 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:45:33.858854 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:45:33.862977 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 07:45:33.864108 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 07:45:33.866941 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 07:45:33.867595 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 07:45:33.878705 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 07:45:33.880545 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 07:45:33.880824 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 07:45:33.881096 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 07:45:33.881238 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 07:45:33.881386 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 07:45:33.881713 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 07:45:33.881804 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 07:45:33.903467 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 07:45:34.936074 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 07:45:34.936756 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 07:45:34.938301 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 07:45:34.944390 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 07:45:34.944982 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 07:45:34.945295 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 07:45:34.948208 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 07:45:34.948707 [ 58 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 07:45:34.949051 [ 58 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 07:45:34.949389 [ 58 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 07:45:34.950415 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): [::1]:2181
2025.07.02 07:45:34.952524 [ 1 ] {} <Information> Application: Listening for Prometheus: http://[::1]:9363
2025.07.02 07:45:34.952804 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 127.0.0.1:2181
2025.07.02 07:45:34.953298 [ 1 ] {} <Information> Application: Listening for Prometheus: http://127.0.0.1:9363
2025.07.02 07:45:34.988901 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:45:34.989857 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:45:34.992176 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:45:34.993058 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 07:45:34.993448 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 07:45:34.994383 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 07:45:43.686899 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:45:53.796178 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:46:03.847167 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:46:13.908040 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:46:23.976095 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:46:34.041338 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:46:43.089716 [ 68 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:46:43.095750 [ 68 ] {} <Error> ConfigReloader: Error updating configuration from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::getRaftConfigurationDiff(Poco::Util::AbstractConfiguration const&, DB::CoordinationSettings const&) const @ 0x000000000e8facbf
3. DB::KeeperServer::getRaftConfigurationDiff(Poco::Util::AbstractConfiguration const&) @ 0x000000000e86de13
4. DB::KeeperDispatcher::updateConfiguration(Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e850036
5. DB::Context::updateKeeperConfiguration(Poco::Util::AbstractConfiguration const&) @ 0x000000000d2afe41
6. void std::__function::__policy_invoker<void (Poco::AutoPtr<Poco::Util::AbstractConfiguration>, bool)>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<DB::Keeper::main(std::vector<String, std::allocator<String>> const&)::$_0, void (Poco::AutoPtr<Poco::Util::AbstractConfiguration>, bool)>>(std::__function::__policy_storage const*, Poco::AutoPtr<Poco::Util::AbstractConfiguration>&&, bool) @ 0x00000000060ebba9
7. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3ce76
8. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
9. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
10. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
11. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
12. ? @ 0x0000000000094ac3
13. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 07:46:44.113240 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:46:54.228396 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:46:58.461378 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 07:46:58.463158 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 07:46:58.463496 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 07:46:58.464839 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 07:46:58.465255 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 07:46:58.573896 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 07:46:58.574324 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 07:46:58.574551 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 07:46:58.574772 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 07:46:58.575027 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 07:46:58.575289 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 07:46:58.575598 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 07:46:58.575923 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 07:46:58.575924 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 07:46:58.576410 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 07:46:58.576625 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 07:46:58.576889 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 07:46:58.577103 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 07:46:58.577298 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 07:46:58.577510 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 07:46:58.577766 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 07:46:58.580776 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 07:46:58.581067 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 3325 us elapsed
2025.07.02 07:46:58.581706 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 07:46:58.582073 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 07:46:58.581764 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 07:46:58.581830 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 07:46:58.581857 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 07:46:58.581948 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 07:46:58.581953 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 07:46:58.581968 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 07:46:58.582006 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 07:46:58.582136 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 07:46:58.582194 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 07:46:58.581746 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 07:46:58.582197 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 07:46:58.582173 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 07:46:58.582259 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 07:46:58.582292 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 07:46:58.586247 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:46:58.586991 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 07:46:58.587721 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:46:58.588752 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:46:58.588991 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:46:58.589246 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:07.749748 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:07.751733 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:07.752472 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:07.754353 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:07.757535 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:07.762330 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:07.762879 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:07.769826 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:07.771679 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:07.772242 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:07.772549 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:07.772823 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:07.773079 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:07.776577 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:07.779814 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:07.780665 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:47:07.781578 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:07.782985 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:07.784196 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:08.632323 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:08.632867 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:08.633057 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:08.633374 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:08.635813 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:08.636159 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:08.636394 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:08.643528 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:08.646615 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:08.646907 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:08.647109 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:08.647316 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:08.647482 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:08.650630 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:08.656196 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:08.657767 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:47:08.660082 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:08.662631 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:08.663063 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:09.609587 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:09.610223 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:09.610727 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:09.611298 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:09.611632 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:09.611934 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:09.612099 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:09.615577 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:09.615924 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:09.616180 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:09.616534 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:09.616697 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:09.616921 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:09.619245 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:09.620336 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:09.620736 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:47:09.621378 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:09.622029 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:09.622490 [ 29 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:10.428490 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:10.428880 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:10.429116 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:10.429520 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:10.429891 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:10.430194 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:10.430449 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:10.434766 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:10.435193 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:10.435437 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:10.435652 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:10.435861 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:10.436169 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:10.438723 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:10.439576 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:10.440347 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:47:10.441001 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:10.441835 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:10.442175 [ 26 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:11.641738 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:11.642214 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:11.642516 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:11.643177 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:11.643653 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:11.645304 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:11.645579 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:11.650462 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:11.650838 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:11.651066 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:11.651287 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:11.651511 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:11.651754 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:11.654096 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:11.654870 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:11.655274 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:47:11.655845 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:11.656489 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:11.656892 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:13.840825 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:13.841786 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:13.842139 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:13.843848 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:13.844311 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:13.844663 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:13.844903 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:13.851743 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:13.852211 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:13.852514 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:13.852778 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:13.853224 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:13.853755 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:13.856172 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:13.857130 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:13.857684 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:47:13.858468 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:13.859550 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:13.860040 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:17.444306 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:17.444915 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:17.445175 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:17.445663 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:17.446161 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:17.446541 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:17.446784 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:17.450842 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:17.451259 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:17.451540 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:17.452028 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:17.452246 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:17.452412 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:17.454198 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:17.454933 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:17.456808 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:47:17.459165 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:17.459815 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:17.460084 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:24.245686 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:24.246211 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:24.246525 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:24.247055 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:24.247466 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:24.247823 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:24.248092 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:24.252289 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:24.252729 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:24.252968 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:24.253283 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:24.253538 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:24.253786 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:24.255941 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:24.256876 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:24.257509 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:47:24.258105 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:24.258702 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:24.259054 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:47:37.421741 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:47:37.422216 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:47:37.422573 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:47:37.422991 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:47:37.423453 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:47:37.423811 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:47:37.424078 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:47:37.427878 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:47:37.428591 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:47:37.429042 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:47:37.429289 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:47:37.430189 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:47:37.430412 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:47:37.432279 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:47:37.433107 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:47:37.433597 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:47:37.434190 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:47:37.434963 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:47:37.435317 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:03.469560 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:03.470012 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:03.470265 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:03.472477 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:03.472982 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:03.473295 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:03.473549 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:03.477373 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:03.477771 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:03.479488 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:03.479679 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:03.479856 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:03.480029 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:03.482524 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:03.483342 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:03.484942 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:48:03.485476 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:03.485977 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:03.486358 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:45.039705 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:45.040832 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:45.042846 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:45.044948 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:45.046685 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:45.054369 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:45.056058 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:45.064107 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:45.064489 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:45.064857 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:45.065182 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:45.065398 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:45.065602 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:45.069154 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:45.069925 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:45.070409 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:48:45.070973 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:45.071623 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:45.071955 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:45.887794 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:45.888595 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:45.888867 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:45.891465 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:45.892196 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:45.893450 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:45.893680 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:45.899155 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:45.899712 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:45.901818 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:45.902212 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:45.903259 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:45.903764 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:45.909948 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:45.910928 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:45.911505 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:48:45.913091 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:45.916221 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:45.916860 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:46.837854 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:46.838362 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:46.838594 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:46.839086 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:46.839698 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:46.840074 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:46.840347 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:46.843211 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:46.843763 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:46.843997 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:46.845789 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:46.846471 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:46.846623 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:46.848880 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:46.849732 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:46.851008 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:48:46.851780 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:46.852499 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:46.852907 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:47.635486 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:47.635886 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:47.636271 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:47.636601 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:47.637073 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:47.637393 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:47.637642 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:47.641249 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:47.642047 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:47.642290 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:47.642558 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:47.642931 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:47.643118 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:47.645059 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:47.646179 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:47.646556 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:48:47.647211 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:47.647760 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:47.648047 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:48.855729 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:48.856113 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:48.856353 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:48.856774 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:48.858416 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:48.858821 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:48.859227 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:48.862443 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:48.863270 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:48.863547 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:48.863799 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:48.864031 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:48.864225 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:48.866348 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:48.867169 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:48.867648 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:48:48.868194 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:48.868741 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:48.869042 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:50.886479 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:50.886926 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:50.887188 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:50.887645 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:50.888014 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:50.888272 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:50.888565 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:50.891660 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:50.892224 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:50.892577 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:50.893035 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:50.893202 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:50.893373 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:50.895046 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:50.895790 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:50.896936 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:48:50.898804 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:50.899621 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:50.900119 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:48:54.544990 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:48:54.545574 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:48:54.545900 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:48:54.546340 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:48:54.546755 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:48:54.547057 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:48:54.547313 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:48:54.550644 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:48:54.551043 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:48:54.551284 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:48:54.551588 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:48:54.551789 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:48:54.552062 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:48:54.554186 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:48:54.555390 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:48:54.557018 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:48:54.557786 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:48:54.558513 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:48:54.559001 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:49:01.359086 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:49:01.360704 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:49:01.361114 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:49:01.361502 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:49:01.361933 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:49:01.362479 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:49:01.362744 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:49:01.366647 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:49:01.367790 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:49:01.371602 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:49:01.372235 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:49:01.372745 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:49:01.373106 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:49:01.375783 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:49:01.376622 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:49:01.377251 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:49:01.377926 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:49:01.379631 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:49:01.380323 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:49:14.564038 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:49:14.564610 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:49:14.564878 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:49:14.565240 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:49:14.565725 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:49:14.566048 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:49:14.566282 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:49:14.569635 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:49:14.570117 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:49:14.570332 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:49:14.570575 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:49:14.570781 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:49:14.571009 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:49:14.573079 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:49:14.574121 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:49:14.574997 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:49:14.575498 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:49:14.576266 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:49:14.576689 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:49:40.565661 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:49:40.566112 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:49:40.566358 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:49:40.566884 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:49:40.567575 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:49:40.567935 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:49:40.568135 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:49:40.571353 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:49:40.571732 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:49:40.571968 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:49:40.572339 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:49:40.572724 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:49:40.573037 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:49:40.574935 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:49:40.575565 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:49:40.575954 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:49:40.576449 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:49:40.577000 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:49:40.577319 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:50:32.209872 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:50:32.210907 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:50:32.211264 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:50:32.211783 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:50:32.212389 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:50:32.213124 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:50:32.213345 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:50:32.216965 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:50:32.217463 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:50:32.217711 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:50:32.217992 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:50:32.218210 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:50:32.218449 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:50:32.220301 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:50:32.221182 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:50:32.221719 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:50:32.222261 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:50:32.222779 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:50:32.223064 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:51:32.603672 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:51:32.604239 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:51:32.604565 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:51:32.604944 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:51:32.605349 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:51:32.605719 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:51:32.605981 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:51:32.609980 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:51:32.610327 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:51:32.610548 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:51:32.610780 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:51:32.611015 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:51:32.611249 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:51:32.613127 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:51:32.614257 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:51:32.614666 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:51:32.615205 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:51:32.615624 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:51:32.615864 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:03.790333 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:03.791585 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:03.792116 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:03.792583 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:03.794328 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:03.796716 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:03.797376 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:03.801144 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:03.804420 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:03.804740 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:03.805113 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:03.805557 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:03.805776 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:03.815397 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:03.815993 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:03.816522 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:03.818169 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:03.818830 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:03.819287 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:04.652005 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:04.652344 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:04.652576 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:04.653074 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:04.654325 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:04.656045 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:04.656276 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:04.659572 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:04.659987 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:04.660176 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:04.660418 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:04.660752 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:04.661124 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:04.662968 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:04.664794 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:04.665293 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:04.666133 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:04.667846 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:04.668561 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:05.519697 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:05.520171 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:05.520918 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:05.521484 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:05.521968 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:05.522348 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:05.522666 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:05.528233 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:05.529221 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:05.529519 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:05.529681 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:05.529839 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:05.529980 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:05.533742 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:05.534605 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:05.536177 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:53:05.537795 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:05.538689 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:05.540160 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:06.319042 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:06.319408 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:06.319630 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:06.320564 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:06.320912 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:06.321227 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:06.321508 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:06.324550 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:06.325482 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:06.325717 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:06.326020 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:06.327565 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:06.327873 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:06.330676 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:06.331346 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:06.331961 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:06.333023 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:06.333747 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:06.334083 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:07.541599 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:07.542071 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:07.542322 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:07.542848 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:07.543302 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:07.543616 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:07.543964 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:07.547525 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:07.547903 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:07.548105 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:07.548395 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:07.548712 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:07.548965 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:07.551000 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:07.551835 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:07.552261 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:07.552830 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:07.553370 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:07.553639 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:09.558601 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:09.559393 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:09.559648 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:09.560086 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:09.560494 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:09.561073 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:09.561298 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:09.564508 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:09.565039 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:09.565884 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:09.566285 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:09.566580 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:09.566799 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:09.569172 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:09.570099 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:09.570626 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:09.571281 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:09.572125 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:09.572383 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:13.181363 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:13.181929 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:13.183571 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:13.184182 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:13.184704 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:13.185039 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:13.185428 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:13.188660 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:13.189020 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:13.189278 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:13.189474 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:13.189803 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:13.189981 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:13.191869 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:13.192827 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:13.193398 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:13.193889 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:13.194471 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:13.194811 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:20.071022 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:20.071416 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:20.071583 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:20.072143 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:20.072675 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:20.073173 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:20.073444 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:20.077159 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:20.077524 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:20.077764 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:20.078036 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:20.078250 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:20.078450 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:20.080425 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:20.081914 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:20.082435 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:20.083081 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:20.083700 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:20.084199 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:33.330555 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:33.331074 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:33.331297 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:33.331678 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:33.345308 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:33.346380 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:33.346854 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:33.351013 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:33.354515 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:33.354745 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:33.354984 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:33.355175 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:33.355880 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:33.361309 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:33.362002 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:33.362496 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:33.364632 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:33.365425 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:33.365677 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:53:59.367183 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:53:59.367696 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:53:59.368693 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:53:59.369254 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:53:59.369851 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:53:59.370179 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:53:59.370404 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:53:59.374306 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:53:59.374793 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:53:59.375212 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:53:59.375708 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:53:59.376253 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:53:59.376485 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:53:59.378472 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:53:59.379423 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:53:59.379892 [ 1 ] {} <Information> Application: Background threads finished in 0 ms
2025.07.02 07:53:59.380679 [ 1 ] {} <Error> Application: Poco::Exception. Code: 1000, e.code() = 0, Not found: keeper_server.raft_configuration.server.hostname, Stack trace (when copying this message, always include the lines below):

0. Poco::Util::AbstractConfiguration::getString(String const&) const @ 0x0000000012bc70e4
1. DB::KeeperStateManager::parseServersConfiguration(Poco::Util::AbstractConfiguration const&, bool, bool) const @ 0x000000000e8f2759
2. DB::KeeperStateManager::KeeperStateManager(int, String const&, String const&, Poco::Util::AbstractConfiguration const&, std::shared_ptr<DB::KeeperContext>) @ 0x000000000e8f674b
3. DB::KeeperServer::KeeperServer(std::shared_ptr<DB::KeeperConfigurationAndSettings> const&, Poco::Util::AbstractConfiguration const&, ConcurrentBoundedQueue<DB::KeeperResponseForSession>&, ConcurrentBoundedQueue<DB::CreateSnapshotTask>&, std::shared_ptr<DB::KeeperContext>, DB::KeeperSnapshotManagerS3&, std::function<void (unsigned long, DB::KeeperRequestForSession const&)>) @ 0x000000000e860270
4. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool, std::shared_ptr<DB::Macros const> const&) @ 0x000000000e8476a7
5. DB::Context::initializeKeeperDispatcher(bool) const @ 0x000000000d2af2f1
6. DB::Keeper::main(std::vector<String, std::allocator<String>> const&) @ 0x00000000060df620
7. Poco::Util::Application::run() @ 0x0000000012bcfba6
8. DB::Keeper::run() @ 0x00000000060dc330
9. mainEntryClickHouseKeeper(int, char**) @ 0x00000000060dafc2
10. main @ 0x00000000060d9b73
11. ? @ 0x0000000000029d90
12. ? @ 0x0000000000029e40
13. _start @ 0x000000000515102e
 (version 25.6.2.5 (official build))
2025.07.02 07:53:59.383044 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:53:59.384907 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:54:43.989890 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:54:43.992741 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:54:43.993232 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:54:43.993786 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:54:44.009952 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:54:44.018781 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:54:44.020134 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:54:44.034401 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:54:44.035050 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:54:44.035229 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:54:44.035444 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:54:44.035648 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:54:44.035856 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:54:44.042405 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 07:54:44.044118 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 07:54:44.046141 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 07:54:44.046851 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 07:54:44.057623 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 07:54:44.058123 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 07:54:44.058386 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 07:54:44.058800 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 07:54:44.059015 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 07:54:44.059295 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 07:54:44.059863 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 07:54:44.060214 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 07:54:44.076753 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 07:54:45.952248 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 07:54:45.952970 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 07:54:45.953470 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 07:54:45.960709 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 07:54:45.961426 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 07:54:45.962976 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 07:54:45.966113 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 07:54:45.966593 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 07:54:45.966881 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 07:54:45.967215 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 07:54:45.970739 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): [::1]:2181
2025.07.02 07:54:45.971352 [ 1 ] {} <Information> Application: Listening for Prometheus: http://[::1]:9363
2025.07.02 07:54:45.971708 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 127.0.0.1:2181
2025.07.02 07:54:45.972087 [ 1 ] {} <Information> Application: Listening for Prometheus: http://127.0.0.1:9363
2025.07.02 07:54:45.983215 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:54:45.984146 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:54:45.984470 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:54:45.985987 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 07:54:45.986281 [ 69 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 07:54:45.986613 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 07:54:53.913737 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:55:03.986221 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:55:14.042150 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:55:24.127738 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:55:34.188250 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:55:43.564360 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 07:55:43.565822 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 07:55:43.566094 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 07:55:43.569253 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 07:55:43.569515 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 07:55:44.031653 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 07:55:44.032078 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 07:55:44.032274 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 07:55:44.032514 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 07:55:44.032903 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 07:55:44.033244 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 07:55:44.033589 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 07:55:44.033941 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 07:55:44.033943 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 07:55:44.034406 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 07:55:44.034620 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 07:55:44.034893 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 07:55:44.035106 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 07:55:44.035350 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 07:55:44.035561 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 07:55:44.035863 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 07:55:44.039175 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 07:55:44.039460 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 3667 us elapsed
2025.07.02 07:55:44.040033 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 07:55:44.040391 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 07:55:44.040165 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 07:55:44.040202 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 07:55:44.040235 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 07:55:44.040294 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 07:55:44.040341 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 07:55:44.040132 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 07:55:44.039825 [ 45 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 07:55:44.040319 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 07:55:44.040495 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 07:55:44.040533 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 07:55:44.040599 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 07:55:44.040629 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 07:55:44.040681 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 07:55:44.040671 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 07:55:44.042233 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 07:55:44.044129 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 07:55:44.044955 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 07:55:44.045580 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 07:55:44.046480 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 07:55:44.047963 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 07:55:44.048175 [ 1 ] {} <Information> Application: shutting down
2025.07.02 07:55:44.048441 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 07:55:52.082797 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 07:55:52.083563 [ 1 ] {} <Information> Application: starting up
2025.07.02 07:55:52.083977 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 07:55:52.085048 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 07:55:52.086838 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:55:52.089392 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 07:55:52.090213 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 07:55:52.095362 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 07:55:52.096341 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 07:55:52.096579 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 07:55:52.096774 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 07:55:52.097385 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 07:55:52.097746 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 07:55:52.108956 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 07:55:52.110588 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 07:55:52.111011 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 07:55:52.111530 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 07:55:52.127473 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 07:55:52.128802 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 07:55:52.129132 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 07:55:52.129373 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 07:55:52.129922 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 07:55:52.130753 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 07:55:52.131224 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 07:55:52.132761 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 07:55:52.152210 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 07:55:53.722868 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 07:55:53.726251 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 07:55:53.726510 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 07:55:53.729056 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 07:55:53.729317 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 07:55:53.729751 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 07:55:53.732369 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 07:55:53.732711 [ 58 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 07:55:53.732951 [ 58 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 07:55:53.733177 [ 58 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 07:55:53.734244 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): [::1]:2181
2025.07.02 07:55:53.734599 [ 1 ] {} <Information> Application: Listening for Prometheus: http://[::1]:9363
2025.07.02 07:55:53.735020 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 127.0.0.1:2181
2025.07.02 07:55:53.735422 [ 1 ] {} <Information> Application: Listening for Prometheus: http://127.0.0.1:9363
2025.07.02 07:55:53.750985 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 07:55:53.751914 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:55:53.752397 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 07:55:53.753190 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 07:55:53.753385 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 07:55:53.753667 [ 69 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 07:56:01.978255 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:56:12.041273 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:56:22.105285 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:56:32.197828 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:56:42.257826 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:56:52.321367 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:57:02.411701 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:57:12.475250 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:57:22.533918 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:57:32.611392 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:57:42.671915 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:57:52.739574 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:58:02.841119 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:58:12.913078 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:58:22.972593 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:58:33.072960 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:58:43.131037 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:58:53.187635 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:59:03.311981 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:59:13.373228 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:59:23.436556 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:59:33.572930 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:59:43.627275 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 07:59:53.687319 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:00:03.782624 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:00:13.838376 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:00:23.967363 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:00:34.021500 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:00:44.079989 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:00:54.164280 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:01:04.232107 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:01:14.292096 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:01:24.375556 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:01:34.435212 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:01:44.490991 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:01:54.570566 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:02:04.635769 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:02:14.702830 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:02:24.803872 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:02:34.862013 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:02:44.935239 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:02:55.054346 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:03:02.498363 [ 68 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:03:02.501247 [ 68 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:02.501705 [ 68 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:04.510084 [ 68 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:03:04.510802 [ 68 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:04.511116 [ 68 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:05.121109 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:03:08.521089 [ 68 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:03:08.522088 [ 68 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:08.522408 [ 68 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:15.182654 [ 30 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:03:22.398517 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:03:22.400521 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:03:22.401546 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:03:22.406237 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:03:22.406768 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:03:22.878389 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:03:22.878849 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:03:22.879331 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:03:22.879637 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:03:22.879930 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:03:22.880366 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:03:22.880580 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:03:22.880961 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:03:22.881197 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:03:22.881869 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:03:22.882170 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:03:22.882609 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:03:22.882917 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:03:22.883314 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:03:22.883894 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:03:22.884595 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:03:22.888809 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:03:22.889278 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 5055 us elapsed
2025.07.02 08:03:22.890893 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:03:22.890566 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:03:22.891428 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:03:22.892133 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:03:22.890977 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:03:22.893089 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:03:22.891708 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:03:22.890938 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:03:22.892152 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:03:22.892862 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:03:22.892568 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:03:22.893021 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:03:22.891729 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:03:22.892707 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:03:22.892776 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:03:22.892387 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:03:22.892839 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:03:22.896937 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:03:22.899043 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:03:22.902669 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:03:22.904711 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:03:22.906599 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:03:22.906996 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:03:22.907592 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:03:32.116742 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:03:32.117461 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:03:32.117884 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:03:32.120003 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:03:32.121582 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:03:32.126763 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:03:32.127272 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:03:32.134435 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:03:32.135418 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:03:32.135678 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:03:32.136223 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:03:32.137281 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:03:32.137853 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:03:32.146148 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:03:32.150529 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:03:32.152663 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:03:32.154571 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:03:32.168835 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:03:32.172664 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:03:32.173388 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:03:32.176699 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:03:32.177071 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:03:32.177481 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:03:32.178327 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:03:32.178882 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:03:32.182072 [ 27 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:03:33.666533 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:03:33.667546 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:03:33.668721 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:03:33.686707 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:03:33.687418 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:03:33.687976 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:03:33.692818 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:03:33.696149 [ 58 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:03:33.697343 [ 58 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:03:33.698034 [ 58 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:03:33.701254 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): [::1]:2181
2025.07.02 08:03:33.705847 [ 1 ] {} <Information> Application: Listening for Prometheus: http://[::1]:9363
2025.07.02 08:03:33.709827 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 127.0.0.1:2181
2025.07.02 08:03:33.714896 [ 1 ] {} <Information> Application: Listening for Prometheus: http://127.0.0.1:9363
2025.07.02 08:03:33.784948 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:03:33.790341 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:33.792760 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:03:33.794686 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:03:33.799154 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:03:33.802245 [ 68 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:03:41.661940 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:03:51.709489 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:04:01.798731 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:04:11.858886 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:04:21.908468 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:04:31.985091 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:04:42.044518 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:04:52.105873 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:05:02.203646 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:05:12.263983 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:05:22.335769 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:05:32.420545 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:05:42.492994 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:05:52.552308 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:06:02.669915 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:06:12.731311 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:06:22.792018 [ 28 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:07:31.470672 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:07:31.471472 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:07:31.471866 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:07:31.472906 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:07:31.473454 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:07:31.476022 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:07:31.476244 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:07:31.485015 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:07:31.486381 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:07:31.486693 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:07:31.486892 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:07:31.487100 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:07:31.487349 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:07:31.497550 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:07:31.499067 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:07:31.500414 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:07:31.501609 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:07:31.510410 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:07:31.511014 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:07:31.511394 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:07:31.511683 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:07:31.511868 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:07:31.512110 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:07:31.512989 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:07:31.513488 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:07:31.528554 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.80 MiB
2025.07.02 08:07:32.655617 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:07:32.656345 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:07:32.657057 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:07:32.661477 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:07:32.661723 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:07:32.661968 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:07:32.665946 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:07:32.666470 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:07:32.666783 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:07:32.667139 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:07:32.668423 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): [::1]:2181
2025.07.02 08:07:32.668907 [ 1 ] {} <Information> Application: Listening for Prometheus: http://[::1]:9363
2025.07.02 08:07:32.669226 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 127.0.0.1:2181
2025.07.02 08:07:32.669521 [ 1 ] {} <Information> Application: Listening for Prometheus: http://127.0.0.1:9363
2025.07.02 08:07:32.685385 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:07:32.688156 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:07:32.688517 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:07:32.689230 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:07:32.690684 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:07:32.690219 [ 68 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:07:40.892607 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:07:50.961439 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:08:01.058085 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:08:11.112307 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:08:21.172266 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:08:31.265876 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:08:41.332415 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:08:51.402886 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:09:01.492333 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:09:11.573213 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:09:21.624149 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:09:31.695511 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:09:41.755085 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:09:51.820651 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:10:01.899210 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:10:11.957563 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:10:22.024407 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:10:26.921852 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:28.931518 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:30.939175 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:32.113168 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:10:32.946188 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:34.952944 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:36.959756 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:38.966917 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:40.976024 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:42.175017 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:10:42.983440 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:44.991191 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:46.998661 [ 69 ] {} <Error> ConfigReloader: Error loading config from '/etc/clickhouse-keeper/keeper_config.xml': Code: 347. DB::Exception: Poco::Exception. Code: 1000, e.code() = 0, SAXParseException: Tag mismatch in '/etc/clickhouse-keeper/keeper_config.xml', line 35 column 6, Stack trace (when copying this message, always include the lines below):

0. Poco::XML::SAXParseException::SAXParseException(String const&, Poco::XML::Locator const&) @ 0x0000000012b17f7e
1. Poco::XML::ParserEngine::handleError(int) @ 0x0000000012b1c8c9
2. Poco::XML::ParserEngine::parse(Poco::XML::InputSource*) @ 0x0000000012b1a83a
3. Poco::XML::SAXParser::parse(String const&) @ 0x0000000012b19ec4
4. Poco::XML::DOMBuilder::parse(String const&) @ 0x0000000012b0c678
5. Poco::XML::DOMParser::parse(String const&) @ 0x0000000012b0bee2
6. DB::ConfigProcessor::parseConfig(String const&, Poco::XML::DOMParser&) @ 0x000000000bf2ac59
7. DB::ConfigProcessor::processConfig(bool*, zkutil::ZooKeeperNodeCache*, std::shared_ptr<Poco::Event> const&, bool) @ 0x000000000bf2b7ce
8. DB::ConfigProcessor::loadConfig(bool, bool) @ 0x000000000bf2ffb7
9. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x000000000bf3c707
10. DB::ConfigReloader::run() @ 0x000000000bf3ee7f
11. void std::__function::__policy_invoker<void ()>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true, true>::ThreadFromGlobalPoolImpl<void (DB::ConfigReloader::*)(), DB::ConfigReloader*>(void (DB::ConfigReloader::*&&)(), DB::ConfigReloader*&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bf40cfd
12. ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000c02b192
13. void* std::__thread_proxy[abi:ne190107]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000c031cba
14. ? @ 0x0000000000094ac3
15. ? @ 0x0000000000125a04
 (version 25.6.2.5 (official build)). (CANNOT_LOAD_CONFIG)
2025.07.02 08:10:47.152424 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:10:47.152887 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:10:47.153059 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:10:47.155049 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:10:47.155282 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:10:47.340453 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:10:47.340890 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:10:47.341208 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:10:47.341380 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:10:47.341640 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:10:47.341945 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:10:47.342155 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:10:47.342367 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:10:47.342368 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:10:47.342857 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:10:47.343081 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:10:47.343314 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:10:47.343491 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:10:47.343743 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:10:47.343955 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:10:47.344214 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:10:47.347641 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:10:47.347987 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 3794 us elapsed
2025.07.02 08:10:47.348586 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:10:47.348838 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:10:47.348455 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:10:47.349475 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:10:47.349797 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:10:47.349117 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:10:47.349227 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:10:47.349260 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:10:47.349292 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:10:47.348635 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:10:47.349347 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:10:47.349360 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:10:47.349433 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:10:47.349484 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:10:47.349521 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:10:47.348858 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:10:47.349031 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:10:47.353828 [ 42 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:10:47.355573 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:10:47.357304 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:10:47.358381 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:10:47.362717 [ 1 ] {} <Information> Application: Background threads finished in 4 ms
2025.07.02 08:10:47.363070 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:10:47.363461 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:14:35.282827 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:14:35.284187 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:14:35.284360 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:14:35.288061 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:14:35.304165 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:14:35.327629 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:14:35.328164 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:14:35.357377 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:14:35.362482 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:14:35.362749 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:14:35.362983 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:14:35.363210 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:14:35.363463 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:14:35.379358 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:14:35.381070 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:14:35.383014 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 89.20 KiB to 26.74 MiB
2025.07.02 08:14:35.387332 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:14:35.392427 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:14:35.445732 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:14:35.447032 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:14:35.447570 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:14:35.451348 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:14:35.451728 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:14:35.452219 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:14:35.455420 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:14:35.456088 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:14:37.360064 [ 46 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:14:37.360741 [ 46 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:14:37.361039 [ 46 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:14:37.377582 [ 46 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:14:37.378073 [ 46 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:14:37.378280 [ 46 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:14:37.397181 [ 46 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:14:37.397750 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:14:37.398065 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:14:37.398337 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:14:37.401396 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:14:37.401847 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:14:37.416818 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:14:37.418462 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:14:37.418853 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:14:37.419784 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:14:37.420299 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:14:37.420644 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:14:37.467267 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:14:37.467267 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:14:37.467376 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:14:37.482299 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 08:14:37.496787 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 08:14:37.514552 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 08:14:46.959240 [ 30 ] {} <Information> KeeperTCPHandler: Got exception processing session #2: Code: 210. DB::NetException: Connection reset by peer, while reading from socket (peer: 172.19.0.7:55802, local: 172.19.0.5:2181). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bf9141b
1. DB::NetException::NetException<String, String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&, String&&) @ 0x000000000c156105
2. DB::ReadBufferFromPocoSocketBase::socketReceiveBytesImpl(char*, unsigned long) @ 0x000000000c155bbc
3. DB::ReadBufferFromPocoSocketBase::nextImpl() @ 0x000000000c156635
4. DB::KeeperTCPHandler::runImpl() @ 0x000000000e3bb170
5. Poco::Net::TCPServerConnection::start() @ 0x0000000012bb2047
6. Poco::Net::TCPServerDispatcher::run() @ 0x0000000012bb2499
7. Poco::PooledThread::run() @ 0x0000000012b7547b
8. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000012b7395d
9. ? @ 0x0000000000094ac3
10. ? @ 0x0000000000125a04

2025.07.02 08:14:49.007755 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:14:49.018335 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 08:14:49.117443 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:14:49.138462 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 08:14:49.297536 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:14:49.323371 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 08:14:59.196522 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:14:59.199193 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:14:59.200159 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:14:59.201024 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:14:59.201297 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:14:59.407829 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:14:59.408386 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:14:59.408612 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:14:59.408877 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:14:59.409123 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:14:59.409359 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:14:59.409591 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:14:59.409891 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:14:59.409894 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:14:59.410422 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:14:59.410671 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:14:59.410908 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:14:59.411101 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 620
2025.07.02 08:14:59.411326 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 620
2025.07.02 08:14:59.411573 [ 1 ] {} <Information> RaftInstance: create snapshot idx 620 log_term 1
2025.07.02 08:14:59.411866 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:14:59.416279 [ 1 ] {} <Information> RaftInstance: snapshot idx 620 log_term 1 created, compact the log store if needed
2025.07.02 08:14:59.416608 [ 1 ] {} <Information> RaftInstance: create snapshot idx 620 log_term 1 done: 4761 us elapsed
2025.07.02 08:14:59.417116 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:14:59.417276 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:14:59.417157 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:14:59.417211 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:14:59.417022 [ 47 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:14:59.417298 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:14:59.417139 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:14:59.417306 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:14:59.417343 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:14:59.417164 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:14:59.417450 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:14:59.417372 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:14:59.417508 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:14:59.417520 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:14:59.417608 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:14:59.417655 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:14:59.418183 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:14:59.421193 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:14:59.422046 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:14:59.423698 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:14:59.424924 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:14:59.426942 [ 1 ] {} <Information> Application: Background threads finished in 2 ms
2025.07.02 08:14:59.427282 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:14:59.428224 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:15:01.394325 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:15:01.394888 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:15:01.395144 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:15:01.395703 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:15:01.400636 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:15:01.406434 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:15:01.407488 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:15:01.412937 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:15:01.413462 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:15:01.414097 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:15:01.414594 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:15:01.415355 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:15:01.415619 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:15:01.424574 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:15:01.426909 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:15:01.427243 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:15:01.429359 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:15:01.446933 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:15:01.448327 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:15:01.448614 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:15:01.448840 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:15:01.449093 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:15:01.449322 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:15:01.451021 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:15:01.451102 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:15:01.463040 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:15:03.088611 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:15:03.089338 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:15:03.089647 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:15:03.091769 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:15:03.092187 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:15:03.092449 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:15:03.095947 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:15:03.096427 [ 58 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:15:03.096835 [ 58 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:15:03.097053 [ 58 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:15:03.097839 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:15:03.098258 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:15:03.114421 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:15:03.115381 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:15:03.115661 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:15:03.116486 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:15:03.116907 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:15:03.116743 [ 66 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:15:03.384322 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:15:03.387203 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 08:15:03.414965 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:15:03.418948 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 08:15:03.508666 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:15:03.511718 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 08:15:11.429757 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:15:11.432388 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 08:15:11.494687 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:15:11.500563 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 08:15:11.592187 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:15:11.598453 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 08:15:21.433619 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9858 ms
2025.07.02 08:15:21.501551 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9849 ms
2025.07.02 08:15:21.599611 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9914 ms
2025.07.02 08:15:31.433611 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:15:31.501550 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10027 ms
2025.07.02 08:15:31.599629 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10027 ms
2025.07.02 08:15:40.145853 [ 60 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 08:15:40.146310 [ 60 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:15:40.146643 [ 60 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 08:15:40.146953 [ 60 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:15:40.147164 [ 60 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 08:15:40.147366 [ 60 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:15:41.455475 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9969 ms
2025.07.02 08:15:41.523484 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9969 ms
2025.07.02 08:15:41.621533 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9968 ms
2025.07.02 08:15:44.075773 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2376 ms
2025.07.02 08:15:47.156515 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 5073 ms
2025.07.02 08:15:47.449816 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3372 ms
2025.07.02 08:15:48.648074 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 5958 ms
2025.07.02 08:15:51.455525 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 4009 ms
2025.07.02 08:15:51.523440 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3829 ms
2025.07.02 08:15:51.621552 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2976 ms
2025.07.02 08:16:01.455501 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10030 ms
2025.07.02 08:16:01.523496 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10030 ms
2025.07.02 08:16:01.621515 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10030 ms
2025.07.02 08:16:11.462047 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10013 ms
2025.07.02 08:16:11.530030 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10012 ms
2025.07.02 08:16:11.628142 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10013 ms
2025.07.02 08:16:21.462092 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10017 ms
2025.07.02 08:16:21.530056 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10016 ms
2025.07.02 08:16:21.628094 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10017 ms
2025.07.02 08:16:31.462142 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10008 ms
2025.07.02 08:16:31.530095 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10008 ms
2025.07.02 08:16:31.628142 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10008 ms
2025.07.02 08:16:41.507286 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9968 ms
2025.07.02 08:16:41.575237 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9966 ms
2025.07.02 08:16:41.673317 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9967 ms
2025.07.02 08:16:51.507328 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10003 ms
2025.07.02 08:16:51.575237 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10004 ms
2025.07.02 08:16:51.673322 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10005 ms
2025.07.02 08:16:53.524997 [ 133 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:17:01.507416 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10027 ms
2025.07.02 08:17:01.575267 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 08:17:01.673401 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10027 ms
2025.07.02 08:17:11.512735 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9992 ms
2025.07.02 08:17:11.580729 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9992 ms
2025.07.02 08:17:11.678771 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9992 ms
2025.07.02 08:17:21.512697 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10008 ms
2025.07.02 08:17:21.580663 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10009 ms
2025.07.02 08:17:21.678754 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10009 ms
2025.07.02 08:17:31.512689 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:17:31.580675 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 08:17:31.678794 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:17:41.543547 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9987 ms
2025.07.02 08:17:41.611465 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9986 ms
2025.07.02 08:17:41.709514 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9986 ms
2025.07.02 08:17:48.106503 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:17:48.109156 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:17:48.109933 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:17:48.112200 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:17:48.112537 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:17:48.266537 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:17:48.266904 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:17:48.267176 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:17:48.267461 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:17:48.267728 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:17:48.268020 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:17:48.268282 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:17:48.268592 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:17:48.268935 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:17:48.269193 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:17:48.269429 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:17:48.269666 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:17:48.269860 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 663
2025.07.02 08:17:48.270052 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 663
2025.07.02 08:17:48.270614 [ 1 ] {} <Information> RaftInstance: create snapshot idx 663 log_term 1
2025.07.02 08:17:48.271185 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:17:48.276567 [ 1 ] {} <Information> RaftInstance: snapshot idx 663 log_term 1 created, compact the log store if needed
2025.07.02 08:17:48.276943 [ 1 ] {} <Information> RaftInstance: create snapshot idx 663 log_term 1 done: 6086 us elapsed
2025.07.02 08:17:48.277603 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:17:48.277343 [ 47 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:17:48.278117 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:17:48.277724 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:17:48.278393 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:17:48.277795 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:17:48.277833 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:17:48.277662 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:17:48.277938 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:17:48.277958 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:17:48.278015 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:17:48.278016 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:17:48.278190 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:17:48.278164 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:17:48.277643 [ 41 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:17:48.278094 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:17:48.277725 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:17:48.281783 [ 40 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:17:48.282513 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:17:48.284255 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:17:48.285768 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:17:48.287239 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:17:48.287508 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:17:48.287924 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:17:54.050612 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:17:54.051107 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:17:54.051355 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:17:54.074410 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:17:54.075276 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:17:54.083334 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:17:54.083661 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:17:54.100138 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:17:54.101103 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:17:54.102142 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:17:54.102317 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:17:54.102601 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:17:54.103079 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:17:54.117561 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:17:54.118867 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:17:54.120478 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:17:54.121728 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:17:54.137845 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:17:54.143705 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:17:54.144284 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:17:54.144740 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:17:54.145069 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:17:54.145395 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:17:54.146457 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 227.06 KiB to 30.45 MiB
2025.07.02 08:17:54.149935 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:17:54.150001 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:17:55.782948 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:17:55.783705 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:17:55.784053 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:17:55.789837 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:17:55.790235 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:17:55.790459 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:17:55.792736 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:17:55.793192 [ 58 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:17:55.793467 [ 58 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:17:55.793822 [ 58 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:17:55.795332 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:17:55.796153 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:17:55.813630 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:17:55.814368 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:17:55.814608 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:17:55.815510 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:17:55.815721 [ 66 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:17:55.819507 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:17:56.013780 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:17:56.014027 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:17:56.018221 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 08:17:56.019858 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 08:17:56.067350 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:17:56.072453 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 08:18:04.054810 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:18:04.060386 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 08:18:04.184372 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:18:04.190458 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 08:18:04.318940 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:18:04.321224 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 08:18:05.211468 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1029 ms
2025.07.02 08:18:14.063924 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 8794 ms
2025.07.02 08:18:14.191653 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9910 ms
2025.07.02 08:18:14.322057 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9909 ms
2025.07.02 08:18:24.064234 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9999 ms
2025.07.02 08:18:24.191669 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10011 ms
2025.07.02 08:18:24.322087 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10011 ms
2025.07.02 08:18:32.822624 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 08:18:32.823236 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:18:32.823577 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 08:18:32.823902 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:18:32.824157 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 08:18:32.824473 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:18:34.083064 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:18:34.210726 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10019 ms
2025.07.02 08:18:34.341129 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:18:37.361137 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3267 ms
2025.07.02 08:18:38.005125 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3780 ms
2025.07.02 08:18:38.727141 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4370 ms
2025.07.02 08:18:38.947927 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1372 ms
2025.07.02 08:18:40.161971 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1430 ms
2025.07.02 08:18:41.261385 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3221 ms
2025.07.02 08:18:43.860831 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 3694 ms
2025.07.02 08:18:44.083138 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 5128 ms
2025.07.02 08:18:44.210750 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2947 ms
2025.07.02 08:18:54.083147 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9994 ms
2025.07.02 08:18:54.210747 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10012 ms
2025.07.02 08:18:54.341128 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10013 ms
2025.07.02 08:19:04.104689 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 08:19:04.232385 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 08:19:04.362706 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 08:19:14.104695 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10002 ms
2025.07.02 08:19:14.232290 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10001 ms
2025.07.02 08:19:14.362691 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10001 ms
2025.07.02 08:19:24.104670 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9957 ms
2025.07.02 08:19:24.232325 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10018 ms
2025.07.02 08:19:24.363122 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10019 ms
2025.07.02 08:19:34.154547 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 08:19:34.282087 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 08:19:34.412967 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:19:44.154491 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9965 ms
2025.07.02 08:19:44.282133 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9968 ms
2025.07.02 08:19:44.412932 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9970 ms
2025.07.02 08:19:54.154661 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10011 ms
2025.07.02 08:19:54.282158 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 08:19:54.412924 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10011 ms
2025.07.02 08:20:04.172845 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:20:04.300503 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 08:20:04.431332 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:20:14.172954 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9979 ms
2025.07.02 08:20:14.300533 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9978 ms
2025.07.02 08:20:14.431411 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9978 ms
2025.07.02 08:20:24.172904 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10011 ms
2025.07.02 08:20:24.300490 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10011 ms
2025.07.02 08:20:24.431287 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10011 ms
2025.07.02 08:20:34.192558 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:20:34.320085 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 08:20:34.450888 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:20:40.935339 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 6595 ms
2025.07.02 08:20:40.975252 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 6504 ms
2025.07.02 08:20:40.987904 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 6775 ms
2025.07.02 08:20:41.911921 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:20:41.912309 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:20:41.912579 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:20:41.915044 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:20:41.915308 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:20:41.984932 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:20:41.985361 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:20:41.985702 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:20:41.986001 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:20:41.986312 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:20:41.986577 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:20:41.986918 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:20:41.987176 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:20:41.987180 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:20:41.987712 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:20:41.987905 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:20:41.988088 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:20:41.988379 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 659
2025.07.02 08:20:41.988634 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 659
2025.07.02 08:20:41.988877 [ 1 ] {} <Information> RaftInstance: create snapshot idx 659 log_term 1
2025.07.02 08:20:41.989168 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:20:41.992381 [ 1 ] {} <Information> RaftInstance: snapshot idx 659 log_term 1 created, compact the log store if needed
2025.07.02 08:20:41.992665 [ 1 ] {} <Information> RaftInstance: create snapshot idx 659 log_term 1 done: 3563 us elapsed
2025.07.02 08:20:41.993229 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:20:41.993259 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:20:41.993287 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:20:41.993280 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:20:41.993332 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:20:41.993367 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:20:41.993466 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:20:41.993550 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:20:41.993604 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:20:41.993650 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:20:41.993050 [ 45 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:20:41.993741 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:20:41.993810 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:20:41.993462 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:20:41.993533 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:20:41.993818 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:20:41.995937 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:20:41.997857 [ 40 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:20:41.998753 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:20:41.999572 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:20:42.000828 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:20:42.002224 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:20:42.002536 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:20:42.002800 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:20:52.771173 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:20:52.777066 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:20:52.777658 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:20:52.778257 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:20:52.779134 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:20:52.798952 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:20:52.803307 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:20:52.817503 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:20:52.819962 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:20:52.820277 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:20:52.820496 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:20:52.820749 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:20:52.820986 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:20:52.831341 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:20:52.832912 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:20:52.833437 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:20:52.834168 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:20:52.853078 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:20:52.854895 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:20:52.855175 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:20:52.855413 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:20:52.855632 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:20:52.855831 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:20:52.856341 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:20:52.856559 [ 58 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:20:52.861029 [ 27 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:20:54.519309 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:20:54.519989 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:20:54.520266 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:20:54.522116 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:20:54.522753 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:20:54.523016 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:20:54.525652 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:20:54.526010 [ 57 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:20:54.526390 [ 57 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:20:54.526761 [ 57 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:20:54.528520 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:20:54.528886 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:20:54.542356 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:20:54.543600 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:20:54.544057 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:20:54.544833 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:20:54.545119 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:20:54.545183 [ 66 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:20:54.699691 [ 28 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:20:54.701036 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:20:54.701834 [ 28 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 08:20:54.703395 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 08:20:54.757276 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:20:54.765871 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 08:21:02.947256 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:21:02.953974 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 08:21:02.962316 [ 28 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:21:02.964477 [ 28 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 08:21:03.055188 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:21:03.057693 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 08:21:12.976140 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9829 ms
2025.07.02 08:21:12.986131 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9840 ms
2025.07.02 08:21:13.079634 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9899 ms
2025.07.02 08:21:16.176215 [ 79 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:21:22.976096 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10008 ms
2025.07.02 08:21:22.986085 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10008 ms
2025.07.02 08:21:23.079686 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10009 ms
2025.07.02 08:21:31.067119 [ 60 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 08:21:31.067646 [ 60 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:21:31.067853 [ 60 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 08:21:31.068042 [ 60 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:21:31.068300 [ 60 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 08:21:31.068516 [ 60 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:21:32.976117 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:21:32.986138 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 08:21:33.079722 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:21:34.692468 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1689 ms
2025.07.02 08:21:35.133135 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1619 ms
2025.07.02 08:21:36.333849 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 3232 ms
2025.07.02 08:21:37.946125 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3244 ms
2025.07.02 08:21:38.526716 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2127 ms
2025.07.02 08:21:39.508645 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1559 ms
2025.07.02 08:21:40.715213 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 5569 ms
2025.07.02 08:21:42.995880 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2279 ms
2025.07.02 08:21:43.005901 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3495 ms
2025.07.02 08:21:43.099463 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4568 ms
2025.07.02 08:21:52.995953 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10013 ms
2025.07.02 08:21:53.005863 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10012 ms
2025.07.02 08:21:53.099495 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10013 ms
2025.07.02 08:22:02.995906 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:22:03.005955 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 08:22:03.099544 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:22:13.029279 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9974 ms
2025.07.02 08:22:13.039213 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9976 ms
2025.07.02 08:22:13.132778 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9975 ms
2025.07.02 08:22:23.029191 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 08:22:23.039195 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 08:22:23.132724 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10001 ms
2025.07.02 08:22:33.029218 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:22:33.039176 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 08:22:33.132732 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:22:43.040178 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9995 ms
2025.07.02 08:22:43.050175 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9996 ms
2025.07.02 08:22:43.143799 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9996 ms
2025.07.02 08:22:53.040209 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10015 ms
2025.07.02 08:22:53.050225 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 08:22:53.143718 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 08:23:03.040174 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 08:23:03.050207 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 08:23:03.143733 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 08:23:13.079827 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9981 ms
2025.07.02 08:23:13.089861 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9981 ms
2025.07.02 08:23:13.183409 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9980 ms
2025.07.02 08:23:23.079814 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10008 ms
2025.07.02 08:23:23.089883 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10009 ms
2025.07.02 08:23:23.183455 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10007 ms
2025.07.02 08:23:33.079826 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:23:33.089883 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 08:23:33.183532 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:23:43.099094 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9985 ms
2025.07.02 08:23:43.109093 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9985 ms
2025.07.02 08:23:43.202610 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9984 ms
2025.07.02 08:23:53.099168 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10010 ms
2025.07.02 08:23:53.109197 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10011 ms
2025.07.02 08:23:53.202712 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10008 ms
2025.07.02 08:24:03.099132 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:24:03.109130 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 08:24:03.202640 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:24:13.122632 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10013 ms
2025.07.02 08:24:13.132648 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10014 ms
2025.07.02 08:24:13.226178 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10013 ms
2025.07.02 08:24:23.122638 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:24:23.132666 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 08:24:23.226202 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:24:33.122640 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10024 ms
2025.07.02 08:24:33.132696 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 08:24:33.226215 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10018 ms
2025.07.02 08:24:43.189989 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9994 ms
2025.07.02 08:24:43.199912 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9993 ms
2025.07.02 08:24:43.293536 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9993 ms
2025.07.02 08:24:53.189926 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10015 ms
2025.07.02 08:24:53.199926 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 08:24:53.293481 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 08:25:03.189932 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:25:03.199912 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 08:25:03.293487 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:25:13.226461 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9981 ms
2025.07.02 08:25:13.236439 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9982 ms
2025.07.02 08:25:13.329962 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9982 ms
2025.07.02 08:25:23.226441 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10010 ms
2025.07.02 08:25:23.236437 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 08:25:23.330068 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10010 ms
2025.07.02 08:25:33.226505 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:25:33.236497 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 08:25:33.330059 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:25:43.246298 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9978 ms
2025.07.02 08:25:43.256296 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9979 ms
2025.07.02 08:25:43.349782 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9979 ms
2025.07.02 08:25:53.246310 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 08:25:53.256171 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10008 ms
2025.07.02 08:25:53.349794 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10008 ms
2025.07.02 08:26:03.246307 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10018 ms
2025.07.02 08:26:03.256209 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10018 ms
2025.07.02 08:26:03.349807 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10018 ms
2025.07.02 08:26:13.260909 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10005 ms
2025.07.02 08:26:13.270835 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 08:26:13.364436 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10006 ms
2025.07.02 08:26:23.260942 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10018 ms
2025.07.02 08:26:23.270827 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10018 ms
2025.07.02 08:26:23.364414 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10018 ms
2025.07.02 08:26:26.777783 [ 79 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:26:33.260908 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9996 ms
2025.07.02 08:26:33.270846 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9996 ms
2025.07.02 08:26:33.364404 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9997 ms
2025.07.02 08:26:36.709343 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 3309 ms
2025.07.02 08:26:37.922007 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 4626 ms
2025.07.02 08:26:40.707642 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7412 ms
2025.07.02 08:26:43.288716 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 5383 ms
2025.07.02 08:26:43.298631 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2597 ms
2025.07.02 08:26:43.310060 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 6618 ms
2025.07.02 08:26:44.832967 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1544 ms
2025.07.02 08:26:47.903793 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 4611 ms
2025.07.02 08:26:49.147352 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1244 ms
2025.07.02 08:26:52.756478 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 8917 ms
2025.07.02 08:26:52.756558 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 7156 ms
2025.07.02 08:26:52.765459 [ 28 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3621 ms
2025.07.02 08:26:53.669669 [ 26 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:26:53.670124 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:26:53.670815 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:26:53.673183 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:26:53.673551 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:26:53.960175 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:26:53.960543 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:26:53.960763 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:26:53.961053 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:26:53.961249 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:26:53.961527 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:26:53.961752 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:26:53.962038 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:26:53.962095 [ 58 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:26:53.962493 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:26:53.963548 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:26:53.963889 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:26:53.964151 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 719
2025.07.02 08:26:53.964546 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 719
2025.07.02 08:26:53.964752 [ 1 ] {} <Information> RaftInstance: create snapshot idx 719 log_term 1
2025.07.02 08:26:53.965095 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:26:53.968298 [ 1 ] {} <Information> RaftInstance: snapshot idx 719 log_term 1 created, compact the log store if needed
2025.07.02 08:26:53.968598 [ 1 ] {} <Information> RaftInstance: create snapshot idx 719 log_term 1 done: 3554 us elapsed
2025.07.02 08:26:53.969054 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:26:53.969578 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:26:53.969633 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:26:53.969171 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:26:53.968949 [ 41 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:26:53.969301 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:26:53.969400 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:26:53.969452 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:26:53.969462 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:26:53.969316 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:26:53.969470 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:26:53.969094 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:26:53.969775 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:26:53.969149 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:26:53.969724 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:26:53.969575 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:26:53.970618 [ 41 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:26:53.973824 [ 37 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:26:53.974823 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:26:53.975762 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:26:53.977467 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:26:53.980488 [ 1 ] {} <Information> Application: Background threads finished in 3 ms
2025.07.02 08:26:53.984103 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:26:53.984544 [ 26 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:27:04.761805 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:27:04.762382 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:27:04.764325 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:27:04.765528 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:27:04.767036 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:27:04.769578 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:27:04.770997 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:27:04.777324 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:27:04.778861 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:27:04.779643 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:27:04.780000 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:27:04.780677 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:27:04.780981 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:27:04.786078 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:27:04.787934 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:27:04.788609 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:27:04.790895 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:27:04.798444 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:27:04.799538 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:27:04.799779 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:27:04.800575 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:27:04.800983 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:27:04.801204 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:27:04.803212 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:27:04.803867 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:27:04.828283 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:27:06.270636 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:27:06.271314 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:27:06.271781 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:27:06.274409 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:27:06.278318 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:27:06.278664 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:27:06.281942 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:27:06.282361 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:27:06.282659 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:27:06.282863 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:27:06.283667 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:27:06.285807 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:27:06.309966 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:27:06.310892 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:27:06.324762 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:27:06.355848 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:27:06.356313 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:27:06.358425 [ 66 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:27:06.415642 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:27:06.419291 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 08:27:06.423579 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:27:06.428231 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 08:27:06.536698 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:27:06.539145 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 08:27:09.760139 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:27:09.766202 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 08:27:09.837533 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:27:09.846601 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 08:27:09.908168 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:27:09.912623 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 08:27:19.767297 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9737 ms
2025.07.02 08:27:19.847509 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9795 ms
2025.07.02 08:27:19.916271 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9850 ms
2025.07.02 08:27:26.705595 [ 80 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:27:29.767316 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10018 ms
2025.07.02 08:27:29.847553 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 08:27:29.915977 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10017 ms
2025.07.02 08:27:38.302597 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 08:27:38.303251 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:27:38.303564 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 08:27:38.303798 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:27:38.304079 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 08:27:38.304320 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:27:39.772435 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9992 ms
2025.07.02 08:27:39.852749 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9992 ms
2025.07.02 08:27:39.921164 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9992 ms
2025.07.02 08:27:41.150325 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1373 ms
2025.07.02 08:27:41.281411 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1355 ms
2025.07.02 08:27:45.570558 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3715 ms
2025.07.02 08:27:48.205421 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 8345 ms
2025.07.02 08:27:49.581884 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 7779 ms
2025.07.02 08:27:49.772396 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 4201 ms
2025.07.02 08:27:59.772435 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10016 ms
2025.07.02 08:27:59.852736 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9984 ms
2025.07.02 08:27:59.921093 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10038 ms
2025.07.02 08:28:09.795469 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9974 ms
2025.07.02 08:28:09.875687 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9974 ms
2025.07.02 08:28:09.944085 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9974 ms
2025.07.02 08:28:19.795445 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10005 ms
2025.07.02 08:28:19.875667 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 08:28:19.944134 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10007 ms
2025.07.02 08:28:29.795435 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10032 ms
2025.07.02 08:28:29.875663 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10032 ms
2025.07.02 08:28:29.944161 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10032 ms
2025.07.02 08:28:39.821676 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9999 ms
2025.07.02 08:28:39.901881 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9999 ms
2025.07.02 08:28:39.970337 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9998 ms
2025.07.02 08:28:49.821686 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9996 ms
2025.07.02 08:28:49.901924 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 08:28:49.970322 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10006 ms
2025.07.02 08:28:59.821714 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10019 ms
2025.07.02 08:28:59.901905 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10019 ms
2025.07.02 08:28:59.970337 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10019 ms
2025.07.02 08:29:09.851887 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9968 ms
2025.07.02 08:29:09.932125 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9968 ms
2025.07.02 08:29:10.000511 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9967 ms
2025.07.02 08:29:19.851896 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9992 ms
2025.07.02 08:29:19.932163 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9993 ms
2025.07.02 08:29:20.000467 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9991 ms
2025.07.02 08:29:29.851864 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10018 ms
2025.07.02 08:29:29.932318 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10018 ms
2025.07.02 08:29:30.000501 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 08:29:39.852219 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10016 ms
2025.07.02 08:29:39.932515 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 08:29:40.000875 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 08:29:49.852252 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 08:29:49.932485 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10009 ms
2025.07.02 08:29:50.000874 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10010 ms
2025.07.02 08:29:59.852276 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9998 ms
2025.07.02 08:29:59.932501 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 08:30:00.000879 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10013 ms
2025.07.02 08:30:09.891459 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9992 ms
2025.07.02 08:30:09.971671 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9992 ms
2025.07.02 08:30:10.040141 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9991 ms
2025.07.02 08:30:19.891458 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9981 ms
2025.07.02 08:30:19.971674 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10002 ms
2025.07.02 08:30:20.040166 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10002 ms
2025.07.02 08:30:29.891470 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9976 ms
2025.07.02 08:30:29.971690 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 08:30:30.040146 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:30:39.914813 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9988 ms
2025.07.02 08:30:39.994990 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9988 ms
2025.07.02 08:30:40.063593 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9988 ms
2025.07.02 08:30:49.914733 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9980 ms
2025.07.02 08:30:49.994991 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9997 ms
2025.07.02 08:30:50.063458 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9997 ms
2025.07.02 08:30:59.914810 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:30:59.995003 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 08:31:00.063510 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:31:04.472110 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 4566 ms
2025.07.02 08:31:04.483719 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4429 ms
2025.07.02 08:31:04.533833 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 4548 ms
2025.07.02 08:31:05.447459 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:31:05.448054 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:31:05.448329 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:31:05.453931 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:31:05.454264 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:31:05.511066 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:31:05.511474 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:31:05.511723 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:31:05.511979 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:31:05.512286 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:31:05.512603 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:31:05.512852 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:31:05.513126 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:31:05.513551 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:31:05.513774 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:31:05.513933 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:31:05.514148 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:31:05.514350 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 689
2025.07.02 08:31:05.514634 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 689
2025.07.02 08:31:05.514902 [ 1 ] {} <Information> RaftInstance: create snapshot idx 689 log_term 1
2025.07.02 08:31:05.515573 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:31:05.520879 [ 1 ] {} <Information> RaftInstance: snapshot idx 689 log_term 1 created, compact the log store if needed
2025.07.02 08:31:05.521191 [ 1 ] {} <Information> RaftInstance: create snapshot idx 689 log_term 1 done: 6028 us elapsed
2025.07.02 08:31:05.521740 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:31:05.521804 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:31:05.521824 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:31:05.521903 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:31:05.521874 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:31:05.521619 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:31:05.521944 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:31:05.521969 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:31:05.521765 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:31:05.521891 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:31:05.521972 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:31:05.521732 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:31:05.521729 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:31:05.522075 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:31:05.522115 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:31:05.522108 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:31:05.523189 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:31:05.525837 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:31:05.526721 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:31:05.528415 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:31:05.529585 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:31:05.530992 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:31:05.531289 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:31:05.531640 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:31:16.392217 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:31:16.394446 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:31:16.394864 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:31:16.396515 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:31:16.397201 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:31:16.402333 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:31:16.402888 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:31:16.410696 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:31:16.413526 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:31:16.413898 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:31:16.414211 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:31:16.414499 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:31:16.418351 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:31:16.434112 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:31:16.436402 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:31:16.437499 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:31:16.439378 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:31:16.469779 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 202.73 KiB to 29.57 MiB
2025.07.02 08:31:16.470809 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:31:16.472833 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:31:16.473108 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:31:16.473297 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:31:16.473478 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:31:16.473706 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:31:16.474270 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:31:16.474707 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:31:18.214512 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:31:18.215110 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:31:18.215313 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:31:18.219139 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:31:18.219445 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:31:18.219770 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:31:18.223172 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:31:18.223757 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:31:18.226351 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:31:18.226673 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:31:18.227450 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:31:18.227800 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:31:18.242496 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:31:18.251116 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:31:18.256646 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:31:18.257448 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:31:18.257724 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:31:18.257893 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:32:30.630326 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:32:30.633540 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:32:30.633872 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:32:30.635972 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:32:30.636400 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:32:30.812392 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:32:30.812929 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:32:30.813272 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:32:30.814660 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:32:30.814909 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:32:30.815285 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:32:30.815503 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:32:30.815869 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:32:30.816096 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:32:30.816338 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:32:30.816600 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:32:30.816944 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:32:30.817119 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:32:30.817474 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:32:30.817763 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:32:30.817997 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:32:30.822797 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:32:30.823212 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 5243 us elapsed
2025.07.02 08:32:30.823821 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:32:30.824105 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:32:30.823921 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:32:30.823969 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:32:30.824030 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:32:30.824143 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:32:30.823908 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:32:30.824209 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:32:30.824262 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:32:30.824322 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:32:30.823822 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:32:30.824640 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:32:30.824705 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:32:30.824869 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:32:30.824902 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:32:30.824971 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:32:30.829407 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:32:30.830256 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:32:30.831627 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:32:30.835976 [ 1 ] {} <Information> Application: Background threads finished in 4 ms
2025.07.02 08:32:30.836556 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:32:30.836969 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:32:42.439492 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:32:42.442674 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:32:42.443061 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:32:42.444478 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:32:42.445805 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:32:42.452199 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:32:42.453636 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:32:42.457799 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:32:42.458391 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:32:42.458773 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:32:42.458989 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:32:42.459210 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:32:42.459386 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:32:42.464032 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:32:42.465376 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:32:42.466159 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:32:42.466807 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:32:42.473537 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:32:42.474212 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:32:42.477904 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:32:42.478460 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:32:42.479151 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:32:42.480407 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:32:42.481690 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:32:42.482008 [ 58 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:32:42.505494 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:32:43.633461 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:32:43.634668 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:32:43.634972 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:32:43.637204 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:32:43.637458 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:32:43.637668 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:32:43.639842 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:32:43.642228 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:32:43.642598 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:32:43.642854 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:32:43.643856 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:32:43.644183 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:32:43.659098 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:32:43.661300 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:32:43.662933 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:32:43.663982 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:32:43.664247 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:32:43.664647 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:39:08.615058 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:39:08.615686 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:39:08.617040 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:39:08.620272 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:39:08.620638 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:39:09.074838 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:39:09.075267 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:39:09.075537 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:39:09.075840 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:39:09.076129 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:39:09.076429 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:39:09.076688 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:39:09.076993 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:39:09.077021 [ 58 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:39:09.077585 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:39:09.077846 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:39:09.078078 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:39:09.078316 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:39:09.078533 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:39:09.078805 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:39:09.079141 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:39:09.082036 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:39:09.082322 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 3273 us elapsed
2025.07.02 08:39:09.082972 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:39:09.083192 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:39:09.083356 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:39:09.083149 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:39:09.082989 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:39:09.083211 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:39:09.082855 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:39:09.083305 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:39:09.083478 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:39:09.083492 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:39:09.083112 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:39:09.083402 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:39:09.083589 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:39:09.083374 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:39:09.083221 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:39:09.083426 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:39:09.084610 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:39:09.087221 [ 40 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:39:09.088014 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:39:09.088787 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:39:09.089951 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:39:09.091547 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:39:09.091925 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:39:09.092349 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:39:20.298243 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:39:20.299137 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:39:20.299620 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:39:20.301116 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:39:20.306958 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:39:20.309976 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:39:20.310971 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:39:20.315133 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:39:20.318173 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:39:20.318408 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:39:20.318577 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:39:20.318752 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:39:20.318939 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:39:20.323196 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:39:20.334623 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:39:20.342228 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:39:20.344856 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:39:20.363673 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 97.31 KiB to 28.01 MiB
2025.07.02 08:39:20.375190 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:39:20.376688 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:39:20.377169 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:39:20.377547 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:39:20.378023 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:39:20.378513 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:39:20.379113 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:39:20.380304 [ 58 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:39:21.603456 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:39:21.604136 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:39:21.604442 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:39:21.607076 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:39:21.607491 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:39:21.608076 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:39:21.616133 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:39:21.618871 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:39:21.619357 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:39:21.619838 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:39:21.621114 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:39:21.622097 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:39:21.659136 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:39:21.683263 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:39:21.683666 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:39:21.684501 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:39:21.684930 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:39:21.684691 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:43:55.684875 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:43:55.687413 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:43:55.691300 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:43:55.693653 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:43:55.694189 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:43:55.974160 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:43:55.974767 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:43:55.975085 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:43:55.975453 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:43:55.975675 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:43:55.975944 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:43:55.976662 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:43:55.976992 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:43:55.977027 [ 58 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:43:55.977523 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:43:55.977961 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:43:55.978301 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:43:55.978607 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:43:55.978958 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:43:55.979162 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:43:55.979378 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:43:55.984086 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:43:55.984472 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 5125 us elapsed
2025.07.02 08:43:55.985215 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:43:55.985693 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:43:55.986051 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:43:55.985383 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:43:55.985554 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:43:55.985228 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:43:55.985183 [ 45 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:43:55.985749 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:43:55.985961 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:43:55.986053 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:43:55.985364 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:43:55.986101 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:43:55.985728 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:43:55.986202 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:43:55.986279 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:43:55.986379 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:43:55.987392 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:43:55.990981 [ 39 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:43:55.992157 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:43:55.993152 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:43:55.994148 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:43:55.996126 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:43:55.996341 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:43:55.997375 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:44:07.545874 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:44:07.547691 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:44:07.547939 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:44:07.549969 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:44:07.554038 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:44:07.560642 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:44:07.561009 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:44:07.569376 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:44:07.570365 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:44:07.570624 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:44:07.570948 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:44:07.571699 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:44:07.572016 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:44:07.577125 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:44:07.579761 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:44:07.580605 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:44:07.581008 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:44:07.591260 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:44:07.591570 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:44:07.591853 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:44:07.592131 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:44:07.592407 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:44:07.592774 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:44:07.593226 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:44:07.593745 [ 61 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:44:07.622250 [ 30 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:44:09.259020 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:44:09.261962 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:44:09.262341 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:44:09.268030 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:44:09.268563 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:44:09.269623 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:44:09.273873 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:44:09.274910 [ 60 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:44:09.275096 [ 60 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:44:09.275315 [ 60 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:44:09.276768 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:44:09.277611 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:44:09.294638 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:44:09.295667 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:44:09.295944 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:44:09.296724 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:44:09.296996 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:44:09.297087 [ 68 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:45:42.657658 [ 29 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:45:42.658877 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:45:42.659327 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:45:42.660397 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:45:42.660778 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:45:42.897282 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:45:42.899014 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:45:42.899702 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:45:42.900098 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:45:42.900389 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:45:42.901646 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:45:42.902219 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:45:42.903352 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:45:42.903270 [ 61 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:45:42.904041 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:45:42.904569 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:45:42.904748 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:45:42.904920 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:45:42.905571 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:45:42.905777 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:45:42.906273 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:45:42.911124 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:45:42.911629 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 5431 us elapsed
2025.07.02 08:45:42.912041 [ 44 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:45:42.912857 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:45:42.912470 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:45:42.912496 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:45:42.912639 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:45:42.912728 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:45:42.912762 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:45:42.912898 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:45:42.912931 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:45:42.912660 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:45:42.912308 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:45:42.913060 [ 59 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:45:42.913105 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:45:42.913159 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:45:42.913215 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:45:42.913242 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:45:42.913310 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:45:42.921273 [ 42 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:45:42.922223 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:45:42.923007 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:45:42.923839 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:45:42.925200 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:45:42.925424 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:45:42.925689 [ 29 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:45:59.150618 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:45:59.154587 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:45:59.155063 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:45:59.155719 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:45:59.159694 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:45:59.162407 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:45:59.162797 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:45:59.166368 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:45:59.168298 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:45:59.168823 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:45:59.169028 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:45:59.169358 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:45:59.169550 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:45:59.176898 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:45:59.177600 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:45:59.178095 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:45:59.181103 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:45:59.196730 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:45:59.197106 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:45:59.197421 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:45:59.197925 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:45:59.198241 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:45:59.198481 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:45:59.201731 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:45:59.202647 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:45:59.219800 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:46:00.498768 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:46:00.499692 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:46:00.499996 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:46:00.502343 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:46:00.502624 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:46:00.502881 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:46:00.505322 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:46:00.506051 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:46:00.506301 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:46:00.506540 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:46:00.508118 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:46:00.508578 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:46:00.527168 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:46:00.529127 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:46:00.529788 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:46:00.530894 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:46:00.531514 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:46:00.531779 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:49:14.429160 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:49:14.430243 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:49:14.432112 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:49:14.436327 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:49:14.436708 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:49:14.785422 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:49:14.785937 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:49:14.786259 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:49:14.786612 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:49:14.786865 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:49:14.787130 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:49:14.787444 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:49:14.788114 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:49:14.788199 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:49:14.788675 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:49:14.788951 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:49:14.789253 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:49:14.789513 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:49:14.789762 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:49:14.790021 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:49:14.790298 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:49:14.793204 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:49:14.793483 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 3217 us elapsed
2025.07.02 08:49:14.793858 [ 45 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:49:14.794272 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:49:14.794591 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:49:14.794225 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:49:14.794329 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:49:14.794191 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:49:14.794399 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:49:14.794436 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:49:14.794461 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:49:14.794511 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:49:14.794497 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:49:14.794185 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:49:14.794542 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:49:14.794718 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:49:14.794222 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:49:14.794778 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:49:14.794758 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:49:14.798543 [ 39 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:49:14.799184 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:49:14.799856 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:49:14.800573 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:49:14.802211 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:49:14.802422 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:49:14.802675 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:49:26.419084 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:49:26.419999 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:49:26.421638 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:49:26.422294 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:49:26.422805 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:49:26.428929 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:49:26.429602 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:49:26.448032 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:49:26.448753 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:49:26.449131 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:49:26.449352 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:49:26.449554 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:49:26.449808 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:49:26.466152 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:49:26.469665 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:49:26.470546 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:49:26.472167 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:49:26.480811 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:49:26.481656 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:49:26.481942 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:49:26.482237 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:49:26.482527 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:49:26.482742 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:49:26.484795 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 235.17 KiB to 30.59 MiB
2025.07.02 08:49:26.487231 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:49:26.487573 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:49:28.095942 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:49:28.104375 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:49:28.105168 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:49:28.108248 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:49:28.108805 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:49:28.109041 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:49:28.112848 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:49:28.114270 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:49:28.114631 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:49:28.114979 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:49:28.116177 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:49:28.117020 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:49:28.131585 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:49:28.147248 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:49:28.147602 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:49:28.149227 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:49:28.149443 [ 66 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:49:28.149643 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:49:46.754470 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:49:46.755961 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:49:46.757016 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:49:46.759805 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:49:46.760352 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:49:47.151389 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:49:47.151848 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:49:47.152124 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:49:47.152378 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:49:47.152682 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:49:47.152879 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:49:47.153102 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:49:47.153530 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:49:47.153662 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:49:47.154123 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:49:47.154346 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:49:47.154560 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:49:47.154814 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:49:47.155073 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:49:47.155335 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:49:47.155729 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:49:47.159001 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:49:47.159366 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 3715 us elapsed
2025.07.02 08:49:47.159882 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:49:47.159901 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:49:47.159917 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:49:47.159944 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:49:47.159999 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:49:47.160049 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:49:47.160099 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:49:47.160147 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:49:47.160156 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:49:47.159796 [ 49 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:49:47.160239 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:49:47.160244 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:49:47.159956 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:49:47.160178 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:49:47.160314 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:49:47.160427 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:49:47.162369 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:49:47.164673 [ 42 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:49:47.165608 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:49:47.166296 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:49:47.167210 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:49:47.168557 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:49:47.168885 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:49:47.169181 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:50:21.810360 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:50:21.811660 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:50:21.811959 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:50:21.823022 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:50:21.823698 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:50:21.833230 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:50:21.843218 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:50:21.852237 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:50:21.853335 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:50:21.853920 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:50:21.854554 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:50:21.855042 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:50:21.856907 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:50:21.875106 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:50:21.876982 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:50:21.877866 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:50:21.879680 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:50:21.892818 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:50:21.901850 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:50:21.902347 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:50:21.903508 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 227.06 KiB to 30.46 MiB
2025.07.02 08:50:21.903665 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:50:21.904699 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:50:21.905666 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:50:21.906014 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:50:21.906400 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:50:23.125863 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:50:23.126697 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:50:23.126967 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:50:23.133090 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:50:23.133436 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:50:23.133716 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:50:23.136547 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:50:23.136897 [ 60 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:50:23.137140 [ 60 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:50:23.137511 [ 60 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:50:23.138216 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:50:23.138517 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:50:23.153636 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:50:23.159148 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:50:23.159646 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:50:23.160683 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:50:23.160959 [ 66 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:50:23.161299 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:54:18.033645 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:54:18.034192 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:54:18.034407 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:54:18.039984 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:54:18.040635 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:54:18.401080 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:54:18.401666 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:54:18.401951 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:54:18.402202 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:54:18.402401 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:54:18.402653 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:54:18.402883 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:54:18.403231 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:54:18.403234 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:54:18.403789 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:54:18.404092 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:54:18.404339 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:54:18.404567 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:54:18.404823 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:54:18.405082 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:54:18.405392 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:54:18.410025 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:54:18.410351 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 4990 us elapsed
2025.07.02 08:54:18.410903 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:54:18.411129 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:54:18.410977 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:54:18.410913 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:54:18.411060 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:54:18.411050 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:54:18.410805 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:54:18.411150 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:54:18.411030 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:54:18.410939 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:54:18.411234 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:54:18.411298 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:54:18.411275 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:54:18.411315 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:54:18.411259 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:54:18.411410 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:54:18.412427 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:54:18.414855 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:54:18.415617 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:54:18.416361 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:54:18.417336 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:54:18.418637 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:54:18.418926 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:54:18.419239 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:54:29.721678 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:54:29.722153 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:54:29.722520 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:54:29.723043 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:54:29.725561 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:54:29.728511 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:54:29.728790 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:54:29.735538 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:54:29.736640 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:54:29.737278 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:54:29.737634 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:54:29.737938 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:54:29.738118 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:54:29.754759 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:54:29.756470 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:54:29.757641 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:54:29.758593 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:54:29.770011 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:54:29.770379 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:54:29.771208 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:54:29.771475 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:54:29.771715 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:54:29.772057 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:54:29.772451 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:54:29.773328 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:54:29.784501 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:54:31.449235 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:54:31.453419 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:54:31.454786 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:54:31.462480 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:54:31.462941 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:54:31.463321 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:54:31.467727 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:54:31.468357 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:54:31.468680 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:54:31.469325 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:54:31.471132 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:54:31.475894 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:54:31.492694 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:54:31.502895 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:54:31.508142 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:54:31.513482 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:54:31.520590 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:54:31.522669 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:55:12.006534 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:55:12.007499 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:55:12.008038 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:55:12.009499 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:55:12.009939 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:55:12.038766 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:55:12.039089 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:55:12.039312 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:55:12.039496 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:55:12.039667 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:55:12.043141 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:55:12.046117 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:55:12.046669 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:55:12.046731 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:55:12.048045 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:55:12.048562 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:55:12.048895 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:55:12.049122 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:55:12.049389 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:55:12.050334 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:55:12.050875 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:55:12.054325 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:55:12.055831 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 4986 us elapsed
2025.07.02 08:55:12.056470 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:55:12.056563 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:55:12.057012 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:55:12.056806 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:55:12.056867 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:55:12.056935 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:55:12.058065 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:55:12.056740 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:55:12.057078 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:55:12.056955 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:55:12.057844 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:55:12.057925 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:55:12.057970 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:55:12.058129 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:55:12.058165 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:55:12.057346 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:55:12.064366 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:55:12.067949 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:55:12.070048 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:55:12.074464 [ 1 ] {} <Information> Application: Background threads finished in 4 ms
2025.07.02 08:55:12.076313 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:55:12.077301 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:55:23.853212 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:55:23.854009 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:55:23.854748 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:55:23.856956 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:55:23.865403 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:55:23.870619 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:55:23.871016 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:55:23.878089 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:55:23.878797 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:55:23.879536 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:55:23.879787 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:55:23.879967 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:55:23.880228 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:55:23.886283 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:55:23.887620 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:55:23.887992 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:55:23.888772 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:55:23.892763 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:55:23.894267 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:55:23.894643 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:55:23.894920 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:55:23.896015 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:55:23.896783 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:55:23.898304 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:55:23.899338 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:55:23.924869 [ 30 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:55:25.306160 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:55:25.307168 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:55:25.307455 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:55:25.310592 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:55:25.310928 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:55:25.311244 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:55:25.315294 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:55:25.315819 [ 60 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:55:25.316089 [ 60 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:55:25.316276 [ 60 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:55:25.316990 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:55:25.317680 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:55:25.336784 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:55:25.337647 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:55:25.337893 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:55:25.339491 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:55:25.339673 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:55:25.340033 [ 68 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:56:50.328301 [ 29 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:56:50.337363 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:56:50.337734 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:56:50.343144 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:56:50.343582 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:56:50.391402 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:56:50.391850 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:56:50.392194 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:56:50.392409 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:56:50.392771 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:56:50.393057 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:56:50.393368 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:56:50.397708 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:56:50.397742 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:56:50.398938 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:56:50.399997 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:56:50.401069 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:56:50.404931 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 08:56:50.405217 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 08:56:50.405402 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 08:56:50.405682 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:56:50.412356 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 08:56:50.412715 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 7126 us elapsed
2025.07.02 08:56:50.414077 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:56:50.414828 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:56:50.414227 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:56:50.414340 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:56:50.414425 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:56:50.414470 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:56:50.414543 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:56:50.414648 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:56:50.414900 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:56:50.414969 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:56:50.415022 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:56:50.415136 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:56:50.414095 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:56:50.415260 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:56:50.415328 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:56:50.415368 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:56:50.422099 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:56:50.425721 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:56:50.432329 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:56:50.437179 [ 1 ] {} <Information> Application: Background threads finished in 4 ms
2025.07.02 08:56:50.438084 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:56:50.440765 [ 29 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:57:07.618907 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:57:07.619321 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:57:07.619625 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:57:07.620131 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:57:07.622158 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:57:07.627173 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:57:07.627635 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:57:07.634711 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:57:07.635396 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:57:07.635601 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:57:07.635889 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:57:07.636025 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:57:07.636189 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:57:07.640265 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:57:07.647824 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:57:07.649574 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:57:07.650373 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:57:07.664084 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:57:07.667365 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:57:07.667936 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:57:07.668226 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:57:07.670559 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:57:07.671993 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:57:07.672549 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:57:07.672910 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:57:07.686924 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:57:09.020472 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:57:09.026391 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:57:09.032094 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:57:09.037834 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:57:09.038301 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:57:09.038676 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:57:09.041209 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:57:09.045108 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:57:09.045582 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:57:09.046394 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:57:09.051426 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:57:09.052365 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:57:09.082733 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:57:09.084308 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:57:09.085350 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:57:09.086489 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:57:09.089843 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:57:09.090347 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:57:14.491235 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:14.500192 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 08:57:14.540017 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:14.542790 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 08:57:14.577230 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:14.580295 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 08:57:18.469403 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:18.472837 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 08:57:18.481133 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:18.488384 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 08:57:18.568787 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:18.578720 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 08:57:28.474010 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9790 ms
2025.07.02 08:57:28.490731 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9784 ms
2025.07.02 08:57:28.579741 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9860 ms
2025.07.02 08:57:38.513648 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9994 ms
2025.07.02 08:57:38.528859 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9993 ms
2025.07.02 08:57:38.570265 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9945 ms
2025.07.02 08:57:39.646740 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 08:57:39.647243 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 08:57:39.647479 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 08:57:39.648597 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 08:57:39.648886 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 08:57:40.117010 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 08:57:40.117850 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 08:57:40.118247 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 08:57:40.118461 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 08:57:40.118886 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 08:57:40.119167 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 08:57:40.119630 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 08:57:40.120027 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 08:57:40.120155 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 08:57:40.120641 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 08:57:40.120889 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 08:57:40.121166 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 08:57:40.121657 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 614
2025.07.02 08:57:40.122089 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 614
2025.07.02 08:57:40.122711 [ 1 ] {} <Information> RaftInstance: create snapshot idx 614 log_term 1
2025.07.02 08:57:40.123073 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 08:57:40.128210 [ 1 ] {} <Information> RaftInstance: snapshot idx 614 log_term 1 created, compact the log store if needed
2025.07.02 08:57:40.128758 [ 1 ] {} <Information> RaftInstance: create snapshot idx 614 log_term 1 done: 5715 us elapsed
2025.07.02 08:57:40.129459 [ 44 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 08:57:40.130466 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 08:57:40.130060 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 08:57:40.131762 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 08:57:40.130378 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 08:57:40.129952 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 08:57:40.130604 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 08:57:40.130700 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 08:57:40.131024 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 08:57:40.131072 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 08:57:40.131316 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 08:57:40.131463 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 08:57:40.131613 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 08:57:40.131762 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 08:57:40.131822 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 08:57:40.130191 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 08:57:40.131850 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 08:57:40.136280 [ 40 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 08:57:40.137610 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 08:57:40.138570 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 08:57:40.140030 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 08:57:40.142009 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 08:57:40.142315 [ 1 ] {} <Information> Application: shutting down
2025.07.02 08:57:40.143205 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 08:57:50.536267 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 08:57:50.540055 [ 1 ] {} <Information> Application: starting up
2025.07.02 08:57:50.541189 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 08:57:50.541853 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 08:57:50.544553 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:57:50.552711 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 08:57:50.553137 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 08:57:50.562014 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 08:57:50.562645 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 08:57:50.563634 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 08:57:50.566215 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 08:57:50.567741 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 08:57:50.568718 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 08:57:50.572772 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 08:57:50.574627 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 08:57:50.575197 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 08:57:50.577662 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 08:57:50.586974 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 08:57:50.587718 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 08:57:50.588758 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 08:57:50.589399 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 08:57:50.590460 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 08:57:50.591243 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 08:57:50.592572 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 08:57:50.593299 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 08:57:50.606806 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 08:57:52.625115 [ 46 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 08:57:52.626195 [ 46 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 08:57:52.626547 [ 46 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 08:57:52.628976 [ 46 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 08:57:52.629397 [ 46 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 08:57:52.629693 [ 46 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 08:57:52.634163 [ 46 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 08:57:52.634706 [ 60 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 08:57:52.635007 [ 60 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 08:57:52.635408 [ 60 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 08:57:52.636888 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 08:57:52.637461 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 08:57:52.661079 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:52.661747 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 08:57:52.662514 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:57:52.662799 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 08:57:52.663711 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 08:57:52.663721 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 08:57:52.663989 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 08:57:52.664880 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 08:57:52.666021 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:52.671614 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 08:57:52.681183 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:52.687600 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 08:57:56.354501 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:56.356968 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 08:57:56.390688 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:56.396682 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 08:57:56.601235 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 08:57:56.603521 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 08:58:06.379604 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9838 ms
2025.07.02 08:58:06.419583 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9869 ms
2025.07.02 08:58:06.626430 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9924 ms
2025.07.02 08:58:11.663270 [ 80 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 08:58:16.379604 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9965 ms
2025.07.02 08:58:16.419629 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9966 ms
2025.07.02 08:58:16.626456 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9965 ms
2025.07.02 08:58:24.668394 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 08:58:24.668833 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:58:24.669196 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 08:58:24.669489 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:58:25.169872 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 08:58:25.170391 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 08:58:26.379573 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 08:58:26.419619 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 08:58:26.626544 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10011 ms
2025.07.02 08:58:29.176545 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2762 ms
2025.07.02 08:58:30.069900 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3698 ms
2025.07.02 08:58:31.759147 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 5144 ms
2025.07.02 08:58:32.121058 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2951 ms
2025.07.02 08:58:33.112614 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3049 ms
2025.07.02 08:58:34.462391 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2346 ms
2025.07.02 08:58:34.987567 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1878 ms
2025.07.02 08:58:35.932848 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4056 ms
2025.07.02 08:58:36.389105 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1393 ms
2025.07.02 08:58:36.429211 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1961 ms
2025.07.02 08:58:46.389159 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9965 ms
2025.07.02 08:58:46.429250 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9970 ms
2025.07.02 08:58:46.636029 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9970 ms
2025.07.02 08:58:56.389113 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10011 ms
2025.07.02 08:58:56.429199 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10011 ms
2025.07.02 08:58:56.636105 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10011 ms
2025.07.02 08:59:06.407137 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 08:59:06.447189 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 08:59:06.654077 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 08:59:16.407200 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9978 ms
2025.07.02 08:59:16.447271 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9978 ms
2025.07.02 08:59:16.654083 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9977 ms
2025.07.02 08:59:26.407231 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10011 ms
2025.07.02 08:59:26.447317 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10012 ms
2025.07.02 08:59:26.654144 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10012 ms
2025.07.02 08:59:36.424883 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 08:59:36.464978 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 08:59:36.671757 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 08:59:46.424893 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9999 ms
2025.07.02 08:59:46.464990 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9999 ms
2025.07.02 08:59:46.671833 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9999 ms
2025.07.02 08:59:56.424857 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9991 ms
2025.07.02 08:59:56.464909 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10017 ms
2025.07.02 08:59:56.671814 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10017 ms
2025.07.02 09:00:06.468443 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10019 ms
2025.07.02 09:00:06.508564 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10019 ms
2025.07.02 09:00:06.715303 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10019 ms
2025.07.02 09:00:16.468365 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9935 ms
2025.07.02 09:00:16.508477 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9935 ms
2025.07.02 09:00:16.715307 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9935 ms
2025.07.02 09:00:26.468407 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10011 ms
2025.07.02 09:00:26.508527 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10032 ms
2025.07.02 09:00:26.715317 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10031 ms
2025.07.02 09:00:36.491445 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10050 ms
2025.07.02 09:00:36.531587 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10050 ms
2025.07.02 09:00:36.738272 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10049 ms
2025.07.02 09:00:46.491428 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9978 ms
2025.07.02 09:00:46.531540 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9978 ms
2025.07.02 09:00:46.738293 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9977 ms
2025.07.02 09:00:56.491454 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:00:56.531541 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:00:56.738308 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10027 ms
2025.07.02 09:01:06.520304 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10015 ms
2025.07.02 09:01:06.560448 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 09:01:06.767274 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:01:16.520323 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9975 ms
2025.07.02 09:01:16.560420 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9975 ms
2025.07.02 09:01:16.767262 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9975 ms
2025.07.02 09:01:26.520471 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10024 ms
2025.07.02 09:01:26.560461 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:01:26.767235 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10024 ms
2025.07.02 09:01:36.550140 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:01:36.590308 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:01:36.797031 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:01:46.550093 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9964 ms
2025.07.02 09:01:46.590240 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9967 ms
2025.07.02 09:01:46.797020 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9967 ms
2025.07.02 09:01:56.550099 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10020 ms
2025.07.02 09:01:56.590223 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 09:01:56.797053 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10020 ms
2025.07.02 09:02:06.578434 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10028 ms
2025.07.02 09:02:06.618568 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 09:02:06.825339 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10028 ms
2025.07.02 09:02:16.578456 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9984 ms
2025.07.02 09:02:16.618532 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9984 ms
2025.07.02 09:02:16.825387 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9983 ms
2025.07.02 09:02:26.578466 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10017 ms
2025.07.02 09:02:26.618520 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10017 ms
2025.07.02 09:02:26.825325 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10018 ms
2025.07.02 09:02:36.613012 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:02:36.653053 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:02:36.859838 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:02:46.612957 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10011 ms
2025.07.02 09:02:46.653059 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10011 ms
2025.07.02 09:02:46.859862 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10011 ms
2025.07.02 09:02:56.612928 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:02:56.653082 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:02:56.859843 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10024 ms
2025.07.02 09:03:06.677573 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:03:06.717612 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:03:06.924410 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:03:16.677701 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9965 ms
2025.07.02 09:03:16.717665 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9966 ms
2025.07.02 09:03:16.924419 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9965 ms
2025.07.02 09:03:26.677524 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10011 ms
2025.07.02 09:03:26.717663 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10011 ms
2025.07.02 09:03:26.924402 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10012 ms
2025.07.02 09:03:34.293910 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7592 ms
2025.07.02 09:03:36.639272 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9976 ms
2025.07.02 09:03:36.726725 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2428 ms
2025.07.02 09:03:36.933480 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:03:37.994917 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1298 ms
2025.07.02 09:03:38.259655 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1313 ms
2025.07.02 09:03:40.150458 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1877 ms
2025.07.02 09:03:41.515182 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3497 ms
2025.07.02 09:03:41.732065 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 4972 ms
2025.07.02 09:03:42.850600 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2687 ms
2025.07.02 09:03:43.838378 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2098 ms
2025.07.02 09:03:46.686745 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 5160 ms
2025.07.02 09:03:46.726723 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2884 ms
2025.07.02 09:03:46.933583 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4076 ms
2025.07.02 09:03:56.132237 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9417 ms
2025.07.02 09:03:56.171165 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9250 ms
2025.07.02 09:03:56.178699 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9503 ms
2025.07.02 09:03:57.184237 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:03:57.186239 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:03:57.186786 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:03:57.188080 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:03:57.188474 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:03:57.555918 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:03:57.556340 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:03:57.556609 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:03:57.556965 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:03:57.557235 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:03:57.557914 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:03:57.558103 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:03:57.558349 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:03:57.558359 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:03:57.558897 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:03:57.559109 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:03:57.559281 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:03:57.559512 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 724
2025.07.02 09:03:57.559731 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 724
2025.07.02 09:03:57.559916 [ 1 ] {} <Information> RaftInstance: create snapshot idx 724 log_term 1
2025.07.02 09:03:57.560179 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:03:57.564266 [ 1 ] {} <Information> RaftInstance: snapshot idx 724 log_term 1 created, compact the log store if needed
2025.07.02 09:03:57.564593 [ 1 ] {} <Information> RaftInstance: create snapshot idx 724 log_term 1 done: 4433 us elapsed
2025.07.02 09:03:57.565491 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:03:57.565674 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:03:57.565714 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:03:57.565822 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:03:57.565826 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:03:57.565894 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:03:57.565529 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:03:57.565942 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:03:57.565570 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:03:57.565949 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:03:57.565349 [ 47 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:03:57.565622 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:03:57.565623 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:03:57.565700 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:03:57.565593 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:03:57.565972 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:03:57.568279 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:03:57.569916 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:03:57.570687 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:03:57.571525 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:03:57.572702 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:03:57.573979 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:03:57.574211 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:03:57.574975 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:04:08.289945 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:04:08.290736 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:04:08.291045 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:04:08.292804 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:04:08.293926 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:04:08.297639 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:04:08.298109 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:04:08.311404 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:04:08.317400 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:04:08.320562 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:04:08.323251 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:04:08.323500 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:04:08.323981 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:04:08.329890 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:04:08.331022 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:04:08.332874 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:04:08.334215 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:04:08.351691 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:04:08.354674 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:04:08.354962 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:04:08.355142 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:04:08.355579 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:04:08.356149 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:04:08.354483 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 227.06 KiB to 30.45 MiB
2025.07.02 09:04:08.356532 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:04:08.356702 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:04:09.488696 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:04:09.492400 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:04:09.492665 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:04:09.496044 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:04:09.496392 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:04:09.496731 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:04:09.499263 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:04:09.499705 [ 58 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:04:09.499955 [ 58 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:04:09.500458 [ 58 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:04:09.501587 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:04:09.502161 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:04:09.540760 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:04:09.542399 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:04:09.543154 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:04:09.552058 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:04:09.554180 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:04:09.552578 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:04:10.035853 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:04:10.041328 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:04:10.061844 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:04:10.067157 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:04:10.080971 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:04:10.088039 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:04:13.231048 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:04:13.238607 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:04:13.260583 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:04:13.264843 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:04:13.383731 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:04:13.388900 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:04:23.239376 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9753 ms
2025.07.02 09:04:23.270622 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9758 ms
2025.07.02 09:04:23.391596 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9867 ms
2025.07.02 09:04:33.239500 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10027 ms
2025.07.02 09:04:33.270670 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:04:33.391557 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10024 ms
2025.07.02 09:04:38.894216 [ 86 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:04:41.536725 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:04:41.537198 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:04:41.537479 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:04:41.537935 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:04:41.538237 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:04:41.538448 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:04:43.263012 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9985 ms
2025.07.02 09:04:43.294242 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9985 ms
2025.07.02 09:04:43.415144 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9984 ms
2025.07.02 09:04:45.227474 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1094 ms
2025.07.02 09:04:46.047078 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2780 ms
2025.07.02 09:04:47.632740 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3460 ms
2025.07.02 09:04:48.113306 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2065 ms
2025.07.02 09:04:49.383904 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1270 ms
2025.07.02 09:04:52.595309 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 4968 ms
2025.07.02 09:04:53.263083 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3884 ms
2025.07.02 09:04:53.415145 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 7285 ms
2025.07.02 09:05:03.263074 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10010 ms
2025.07.02 09:05:03.294262 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:05:03.415268 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10024 ms
2025.07.02 09:05:13.283093 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10010 ms
2025.07.02 09:05:13.314216 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 09:05:13.435132 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10009 ms
2025.07.02 09:05:23.283084 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10018 ms
2025.07.02 09:05:23.314249 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10018 ms
2025.07.02 09:05:23.435129 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10019 ms
2025.07.02 09:05:33.283081 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10024 ms
2025.07.02 09:05:33.314225 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:05:33.435151 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10024 ms
2025.07.02 09:05:43.343264 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9977 ms
2025.07.02 09:05:43.374409 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9977 ms
2025.07.02 09:05:43.495363 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9976 ms
2025.07.02 09:05:53.343353 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10003 ms
2025.07.02 09:05:53.374474 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10003 ms
2025.07.02 09:05:53.495358 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10003 ms
2025.07.02 09:06:01.751378 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 8395 ms
2025.07.02 09:06:01.790193 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 8314 ms
2025.07.02 09:06:01.810554 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 8486 ms
2025.07.02 09:06:02.800005 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:06:02.801273 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:06:02.801568 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:06:02.804662 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:06:02.805637 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:06:03.137916 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:06:03.138490 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:06:03.138795 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:06:03.139210 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:06:03.139426 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:06:03.139726 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:06:03.139936 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:06:03.140174 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:06:03.140189 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:06:03.140887 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:06:03.141237 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:06:03.141426 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:06:03.141756 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 647
2025.07.02 09:06:03.142216 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 647
2025.07.02 09:06:03.142419 [ 1 ] {} <Information> RaftInstance: create snapshot idx 647 log_term 1
2025.07.02 09:06:03.142720 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:06:03.146453 [ 1 ] {} <Information> RaftInstance: snapshot idx 647 log_term 1 created, compact the log store if needed
2025.07.02 09:06:03.146776 [ 1 ] {} <Information> RaftInstance: create snapshot idx 647 log_term 1 done: 4159 us elapsed
2025.07.02 09:06:03.147632 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:06:03.147716 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:06:03.148264 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:06:03.147852 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:06:03.147811 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:06:03.147690 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:06:03.147742 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:06:03.147955 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:06:03.148039 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:06:03.148118 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:06:03.148112 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:06:03.147830 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:06:03.148357 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:06:03.148377 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:06:03.148406 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:06:03.148445 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:06:03.151923 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:06:03.152704 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:06:03.153680 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:06:03.154922 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:06:03.155174 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:06:03.156164 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:06:13.826310 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:06:13.827849 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:06:13.829260 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:06:13.830279 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:06:13.831174 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:06:13.836626 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:06:13.837930 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:06:13.849547 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:06:13.850438 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:06:13.851110 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:06:13.851613 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:06:13.852835 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:06:13.853266 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:06:13.866959 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:06:13.874554 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:06:13.875176 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:06:13.876038 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:06:13.888746 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:06:13.889549 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:06:13.889972 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:06:13.890168 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:06:13.894407 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:06:13.892368 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 227.06 KiB to 30.46 MiB
2025.07.02 09:06:13.901617 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:06:13.904809 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:06:13.905707 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:06:15.521542 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:06:15.522204 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:06:15.523253 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:06:15.526307 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:06:15.526963 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:06:15.527886 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:06:15.538238 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:06:15.538949 [ 60 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:06:15.540044 [ 60 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:06:15.540321 [ 60 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:06:15.542475 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:06:15.543709 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:06:15.559298 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:06:15.561304 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:06:15.562034 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:06:15.562817 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:06:15.563047 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:06:15.563303 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:06:15.799374 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:06:15.801512 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:06:15.854966 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:06:15.859410 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:06:15.873429 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:06:15.877036 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:06:23.612054 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:06:23.624181 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:06:23.652135 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:06:23.657136 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:06:23.787876 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:06:23.790411 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:06:33.627528 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9895 ms
2025.07.02 09:06:33.658150 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9910 ms
2025.07.02 09:06:33.791818 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9937 ms
2025.07.02 09:06:35.592799 [ 80 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:06:43.659033 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9983 ms
2025.07.02 09:06:43.689610 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9983 ms
2025.07.02 09:06:43.823376 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9982 ms
2025.07.02 09:06:52.090289 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:06:52.090616 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:06:52.090932 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:06:52.091197 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:06:52.091461 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:06:52.091686 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:06:53.659018 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10005 ms
2025.07.02 09:06:53.689568 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10005 ms
2025.07.02 09:06:53.823332 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10006 ms
2025.07.02 09:06:55.822149 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2000 ms
2025.07.02 09:06:55.979978 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2204 ms
2025.07.02 09:07:00.501019 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4689 ms
2025.07.02 09:07:00.543703 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 5902 ms
2025.07.02 09:07:01.582240 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1040 ms
2025.07.02 09:07:03.225157 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2730 ms
2025.07.02 09:07:03.302265 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7340 ms
2025.07.02 09:07:03.659035 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2081 ms
2025.07.02 09:07:13.678025 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9975 ms
2025.07.02 09:07:13.708594 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9975 ms
2025.07.02 09:07:13.842436 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9974 ms
2025.07.02 09:07:23.678005 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10004 ms
2025.07.02 09:07:23.708585 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10004 ms
2025.07.02 09:07:23.842343 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10004 ms
2025.07.02 09:07:33.678073 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10027 ms
2025.07.02 09:07:33.708598 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10027 ms
2025.07.02 09:07:33.842358 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10027 ms
2025.07.02 09:07:43.693033 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10003 ms
2025.07.02 09:07:43.723679 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10003 ms
2025.07.02 09:07:43.857270 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10003 ms
2025.07.02 09:07:53.692986 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10015 ms
2025.07.02 09:07:53.723689 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10016 ms
2025.07.02 09:07:53.857331 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10016 ms
2025.07.02 09:08:03.693030 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:08:03.723592 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:08:03.857365 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:08:13.739561 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9978 ms
2025.07.02 09:08:13.770150 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9978 ms
2025.07.02 09:08:13.903878 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9978 ms
2025.07.02 09:08:23.739583 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10007 ms
2025.07.02 09:08:23.770122 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 09:08:23.903854 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10007 ms
2025.07.02 09:08:33.739544 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10031 ms
2025.07.02 09:08:33.770200 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10031 ms
2025.07.02 09:08:33.903845 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10031 ms
2025.07.02 09:08:43.765089 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9986 ms
2025.07.02 09:08:43.795920 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9986 ms
2025.07.02 09:08:43.929332 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9985 ms
2025.07.02 09:08:53.765054 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10001 ms
2025.07.02 09:08:53.795557 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10001 ms
2025.07.02 09:08:53.929329 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10000 ms
2025.07.02 09:09:02.853124 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9077 ms
2025.07.02 09:09:02.858223 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 8948 ms
2025.07.02 09:09:02.860966 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9115 ms
2025.07.02 09:09:03.855145 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:09:03.856418 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:09:03.856650 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:09:03.858709 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:09:03.859122 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:09:04.230041 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:09:04.230494 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:09:04.230779 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:09:04.231002 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:09:04.231210 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:09:04.231825 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:09:04.232191 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:09:04.232501 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:09:04.232523 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:09:04.232937 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:09:04.233258 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:09:04.233517 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:09:04.233772 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 653
2025.07.02 09:09:04.234032 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 653
2025.07.02 09:09:04.234434 [ 1 ] {} <Information> RaftInstance: create snapshot idx 653 log_term 1
2025.07.02 09:09:04.234803 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:09:04.238740 [ 1 ] {} <Information> RaftInstance: snapshot idx 653 log_term 1 created, compact the log store if needed
2025.07.02 09:09:04.239093 [ 1 ] {} <Information> RaftInstance: create snapshot idx 653 log_term 1 done: 4378 us elapsed
2025.07.02 09:09:04.239902 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:09:04.239936 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:09:04.239941 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:09:04.239998 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:09:04.240067 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:09:04.240041 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:09:04.240224 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:09:04.240259 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:09:04.240350 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:09:04.240445 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:09:04.240363 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:09:04.240443 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:09:04.240549 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:09:04.240471 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:09:04.240568 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:09:04.240680 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:09:04.244767 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:09:04.245794 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:09:04.247064 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:09:04.248556 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:09:04.248957 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:09:04.249314 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:09:14.838956 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:09:14.841742 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:09:14.842105 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:09:14.843804 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:09:14.848884 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:09:14.852126 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:09:14.852570 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:09:14.867369 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:09:14.872490 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:09:14.874410 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:09:14.876933 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:09:14.878037 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:09:14.878887 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:09:14.886918 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:09:14.889040 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:09:14.890042 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:09:14.891185 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:09:14.902288 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:09:14.906157 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:09:14.906540 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:09:14.908322 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:09:14.907553 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 227.06 KiB to 30.46 MiB
2025.07.02 09:09:14.910235 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:09:14.910863 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:09:14.912368 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:09:14.913153 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:09:16.668723 [ 49 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:09:16.669687 [ 49 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:09:16.670164 [ 49 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:09:16.672491 [ 49 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:09:16.672909 [ 49 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:09:16.673168 [ 49 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:09:16.682758 [ 49 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:09:16.683161 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:09:16.683561 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:09:16.683931 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:09:16.685150 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:09:16.685732 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:09:16.697078 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:09:16.697815 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:09:16.698268 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:09:16.699158 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:09:16.699471 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:09:16.699487 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:09:16.826780 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:09:16.826856 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:09:16.829652 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:09:16.831031 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:09:16.894560 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:09:16.900262 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:09:24.996911 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:09:25.002008 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:09:25.128524 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:09:25.131586 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:09:25.144484 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:09:25.146860 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:09:35.003571 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9917 ms
2025.07.02 09:09:35.134938 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9839 ms
2025.07.02 09:09:35.151115 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9848 ms
2025.07.02 09:09:36.383298 [ 80 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:09:37.078371 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2068 ms
2025.07.02 09:09:37.087840 [ 33 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 6 took 1932 ms
2025.07.02 09:09:37.087840 [ 30 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 5 took 1948 ms
2025.07.02 09:09:45.011996 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 7516 ms
2025.07.02 09:09:45.143276 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7691 ms
2025.07.02 09:09:45.159511 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 7710 ms
2025.07.02 09:09:53.207683 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:09:53.208108 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:09:53.208452 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:09:53.208815 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:09:53.208975 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:09:53.209268 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:09:55.011997 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 09:09:55.143270 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10009 ms
2025.07.02 09:09:55.159554 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10009 ms
2025.07.02 09:09:57.390378 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2382 ms
2025.07.02 09:09:59.994290 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2609 ms
2025.07.02 09:10:00.129018 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 4997 ms
2025.07.02 09:10:01.162439 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 5044 ms
2025.07.02 09:10:02.417850 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2429 ms
2025.07.02 09:10:04.078341 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3959 ms
2025.07.02 09:10:05.012038 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2600 ms
2025.07.02 09:10:05.159561 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4007 ms
2025.07.02 09:10:15.036180 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9993 ms
2025.07.02 09:10:15.167449 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9993 ms
2025.07.02 09:10:15.183694 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9992 ms
2025.07.02 09:10:25.036238 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10012 ms
2025.07.02 09:10:25.167431 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10012 ms
2025.07.02 09:10:25.183666 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10012 ms
2025.07.02 09:10:35.036205 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10024 ms
2025.07.02 09:10:35.167416 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:10:35.183682 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 09:10:45.073545 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10003 ms
2025.07.02 09:10:45.204775 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10003 ms
2025.07.02 09:10:45.220931 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10003 ms
2025.07.02 09:10:55.073516 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10017 ms
2025.07.02 09:10:55.204703 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10017 ms
2025.07.02 09:10:55.220984 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10017 ms
2025.07.02 09:11:05.073548 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:11:05.204723 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:11:05.220926 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:11:15.127051 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10000 ms
2025.07.02 09:11:15.258262 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9999 ms
2025.07.02 09:11:15.274545 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10000 ms
2025.07.02 09:11:25.127044 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10016 ms
2025.07.02 09:11:25.258317 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 09:11:25.274564 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 09:11:35.127091 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:11:35.258283 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:11:35.274543 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:11:45.174549 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9949 ms
2025.07.02 09:11:45.305692 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9949 ms
2025.07.02 09:11:45.321933 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9948 ms
2025.07.02 09:11:55.174430 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9992 ms
2025.07.02 09:11:55.305672 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9993 ms
2025.07.02 09:11:55.321933 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9993 ms
2025.07.02 09:12:05.174412 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 09:12:05.305641 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 09:12:05.321864 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 09:12:15.173622 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:12:15.304741 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:12:15.321159 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:12:25.173593 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10014 ms
2025.07.02 09:12:25.304837 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10013 ms
2025.07.02 09:12:25.321206 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:12:35.173625 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:12:35.304846 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:12:35.321240 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:12:45.214847 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9996 ms
2025.07.02 09:12:45.346056 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 09:12:45.362414 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10006 ms
2025.07.02 09:12:55.215159 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10019 ms
2025.07.02 09:12:55.346225 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10019 ms
2025.07.02 09:12:55.362494 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10016 ms
2025.07.02 09:13:05.215115 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10004 ms
2025.07.02 09:13:05.346218 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10004 ms
2025.07.02 09:13:05.362565 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10004 ms
2025.07.02 09:13:15.249217 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9985 ms
2025.07.02 09:13:15.380358 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9986 ms
2025.07.02 09:13:15.396717 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9985 ms
2025.07.02 09:13:25.249258 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10013 ms
2025.07.02 09:13:25.380457 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10013 ms
2025.07.02 09:13:25.396808 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:13:35.249507 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:13:35.380431 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:13:35.396826 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 09:13:45.278893 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9961 ms
2025.07.02 09:13:45.410109 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9961 ms
2025.07.02 09:13:45.426474 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9961 ms
2025.07.02 09:13:55.278961 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10004 ms
2025.07.02 09:13:55.410135 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10018 ms
2025.07.02 09:13:55.426458 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10019 ms
2025.07.02 09:14:05.278932 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10030 ms
2025.07.02 09:14:05.410093 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 09:14:05.426565 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10029 ms
2025.07.02 09:14:15.296683 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9977 ms
2025.07.02 09:14:15.428013 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9976 ms
2025.07.02 09:14:15.444288 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9976 ms
2025.07.02 09:14:25.296695 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 09:14:25.427943 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10009 ms
2025.07.02 09:14:25.444298 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10010 ms
2025.07.02 09:14:35.296893 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10032 ms
2025.07.02 09:14:35.428024 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10032 ms
2025.07.02 09:14:35.444346 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10032 ms
2025.07.02 09:14:45.322595 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9915 ms
2025.07.02 09:14:45.453841 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9978 ms
2025.07.02 09:14:45.470210 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9977 ms
2025.07.02 09:14:55.322568 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10008 ms
2025.07.02 09:14:55.453804 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10009 ms
2025.07.02 09:14:55.470199 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10009 ms
2025.07.02 09:14:58.730786 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3283 ms
2025.07.02 09:14:58.749910 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 3286 ms
2025.07.02 09:14:58.753436 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3437 ms
2025.07.02 09:14:59.891909 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:14:59.893225 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:14:59.893966 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:14:59.896409 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:14:59.897015 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:15:00.070665 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:15:00.071072 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:15:00.071372 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:15:00.071630 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:15:00.071905 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:15:00.072188 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:15:00.072450 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:15:00.072775 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:15:00.073019 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:15:00.073529 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:15:00.073804 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:15:00.074069 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:15:00.074284 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 784
2025.07.02 09:15:00.074555 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 784
2025.07.02 09:15:00.074807 [ 1 ] {} <Information> RaftInstance: create snapshot idx 784 log_term 1
2025.07.02 09:15:00.075286 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:15:00.083830 [ 1 ] {} <Information> RaftInstance: snapshot idx 784 log_term 1 created, compact the log store if needed
2025.07.02 09:15:00.084182 [ 1 ] {} <Information> RaftInstance: create snapshot idx 784 log_term 1 done: 9146 us elapsed
2025.07.02 09:15:00.085737 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:15:00.086624 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:15:00.086938 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:15:00.085808 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:15:00.086175 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:15:00.086222 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:15:00.086109 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:15:00.086316 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:15:00.086389 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:15:00.085704 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:15:00.086692 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:15:00.086777 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:15:00.086949 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:15:00.087022 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:15:00.085852 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:15:00.087085 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:15:00.091577 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:15:00.097431 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:15:00.099532 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:15:00.101633 [ 1 ] {} <Information> Application: Background threads finished in 2 ms
2025.07.02 09:15:00.101952 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:15:00.102898 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:15:11.013122 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:15:11.014972 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:15:11.015316 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:15:11.016460 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:15:11.019188 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:15:11.021971 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:15:11.023364 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:15:11.031397 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:15:11.032703 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:15:11.032985 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:15:11.033232 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:15:11.033891 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:15:11.034156 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:15:11.051294 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:15:11.052958 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:15:11.054290 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:15:11.055024 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:15:11.063232 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:15:11.063835 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:15:11.064265 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:15:11.064495 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:15:11.064710 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:15:11.064867 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:15:11.065172 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:15:11.065449 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:15:11.077494 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 09:15:12.143954 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:15:12.144783 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:15:12.145137 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:15:12.150435 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:15:12.150803 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:15:12.151254 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:15:12.156552 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:15:12.157187 [ 60 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:15:12.157708 [ 60 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:15:12.158041 [ 60 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:15:12.162026 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:15:12.162577 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:15:12.240702 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:15:12.241686 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:15:12.242131 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:15:12.243437 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:15:12.245965 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:15:12.246091 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:15:13.526223 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:15:13.526468 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:15:13.531010 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:15:13.533709 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:15:13.558894 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:15:13.566348 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:15:16.729616 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:15:16.734715 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:15:16.822848 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:15:16.825558 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:15:16.874549 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:15:16.876651 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:15:26.735893 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9841 ms
2025.07.02 09:15:26.826587 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9833 ms
2025.07.02 09:15:26.877399 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9875 ms
2025.07.02 09:15:32.260787 [ 80 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:15:32.962851 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 6240 ms
2025.07.02 09:15:32.974589 [ 31 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 6 took 6112 ms
2025.07.02 09:15:32.974604 [ 30 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 5 took 6162 ms
2025.07.02 09:15:36.735977 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2770 ms
2025.07.02 09:15:36.826706 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2846 ms
2025.07.02 09:15:36.877448 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2893 ms
2025.07.02 09:15:45.209156 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:15:45.209797 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:15:45.210600 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:15:45.210851 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:15:45.211133 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:15:45.211400 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:15:46.769935 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9962 ms
2025.07.02 09:15:46.860596 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9961 ms
2025.07.02 09:15:46.911457 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9961 ms
2025.07.02 09:15:47.921434 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1149 ms
2025.07.02 09:15:48.296249 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1433 ms
2025.07.02 09:15:49.271907 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:15:49.272733 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:15:49.273009 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:15:49.274444 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:15:49.274968 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:15:49.713182 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:15:49.713643 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:15:49.713987 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:15:49.714204 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:15:49.714652 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:15:49.715069 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:15:49.715279 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:15:49.715594 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:15:49.715699 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:15:49.716242 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:15:49.716512 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:15:49.716749 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:15:49.716964 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 712
2025.07.02 09:15:49.717397 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 712
2025.07.02 09:15:49.717727 [ 1 ] {} <Information> RaftInstance: create snapshot idx 712 log_term 1
2025.07.02 09:15:49.718046 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:15:49.722387 [ 1 ] {} <Information> RaftInstance: snapshot idx 712 log_term 1 created, compact the log store if needed
2025.07.02 09:15:49.722737 [ 1 ] {} <Information> RaftInstance: create snapshot idx 712 log_term 1 done: 4761 us elapsed
2025.07.02 09:15:49.724327 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:15:49.724619 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:15:49.724864 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:15:49.725245 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:15:49.724458 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:15:49.724808 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:15:49.724726 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:15:49.724306 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:15:49.724954 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:15:49.725025 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:15:49.725062 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:15:49.725092 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:15:49.724400 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:15:49.725116 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:15:49.724115 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:15:49.725414 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:15:49.728450 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:15:49.729218 [ 42 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:15:49.730062 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:15:49.731689 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:15:49.733318 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:15:49.734777 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:15:49.735081 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:15:49.735421 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:16:00.396878 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:16:00.397450 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:16:00.397720 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:16:00.398514 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:16:00.400778 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:16:00.403152 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:16:00.404547 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:16:00.419510 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:16:00.420371 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:16:00.421272 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:16:00.421835 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:16:00.425113 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:16:00.425832 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:16:00.430621 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:16:00.431510 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:16:00.432206 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:16:00.437435 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:16:00.457066 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:16:00.457470 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:16:00.457718 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:16:00.457940 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:16:00.458311 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:16:00.458646 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:16:00.461554 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:16:00.464666 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:16:00.465415 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 29.29 MiB
2025.07.02 09:16:02.031191 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:16:02.032023 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:16:02.032404 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:16:02.036245 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:16:02.036950 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:16:02.037192 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:16:02.042660 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:16:02.043302 [ 55 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:16:02.043622 [ 55 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:16:02.043955 [ 55 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:16:02.046619 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:16:02.047623 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:16:02.068107 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:16:02.072521 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:16:02.111074 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:16:02.112897 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:16:02.114002 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:16:02.115612 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:16:02.115931 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:16:02.116020 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:16:02.145503 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:16:02.148910 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:16:02.158440 [ 34 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:16:02.167145 [ 34 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:16:05.274671 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:16:05.277178 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:16:05.277466 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:16:05.280179 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:16:05.388792 [ 34 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:16:05.392988 [ 34 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:16:15.296015 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9754 ms
2025.07.02 09:16:15.300995 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9751 ms
2025.07.02 09:16:15.412447 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9832 ms
2025.07.02 09:16:25.295095 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10012 ms
2025.07.02 09:16:25.301032 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 09:16:25.412414 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 09:16:30.409732 [ 86 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:16:31.133489 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 5842 ms
2025.07.02 09:16:31.169097 [ 34 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 6 took 5769 ms
2025.07.02 09:16:31.169333 [ 30 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 4 took 5883 ms
2025.07.02 09:16:33.571903 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:16:33.572449 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:16:33.572777 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:16:33.573204 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:16:33.573456 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:16:33.573711 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:16:35.295143 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3066 ms
2025.07.02 09:16:35.300965 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3087 ms
2025.07.02 09:16:35.412450 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 3189 ms
2025.07.02 09:16:37.253465 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1961 ms
2025.07.02 09:16:41.692934 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 6214 ms
2025.07.02 09:16:42.414386 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7043 ms
2025.07.02 09:16:44.377980 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2672 ms
2025.07.02 09:16:44.639348 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 6386 ms
2025.07.02 09:16:45.341978 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1890 ms
2025.07.02 09:16:55.336239 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9999 ms
2025.07.02 09:16:55.342076 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9999 ms
2025.07.02 09:16:55.453600 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10000 ms
2025.07.02 09:17:05.336138 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 09:17:05.342016 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:17:05.453502 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:17:15.335326 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9987 ms
2025.07.02 09:17:15.341180 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9988 ms
2025.07.02 09:17:15.452699 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9987 ms
2025.07.02 09:17:25.335363 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 09:17:25.341170 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10008 ms
2025.07.02 09:17:25.452754 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10008 ms
2025.07.02 09:17:35.335367 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:17:35.341195 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:17:35.452721 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:17:45.356415 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9978 ms
2025.07.02 09:17:45.362251 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9979 ms
2025.07.02 09:17:45.473790 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9978 ms
2025.07.02 09:17:55.356413 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 09:17:55.362227 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10009 ms
2025.07.02 09:17:55.473814 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10009 ms
2025.07.02 09:18:05.356733 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10028 ms
2025.07.02 09:18:05.362254 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 09:18:05.473873 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10028 ms
2025.07.02 09:18:15.380523 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9990 ms
2025.07.02 09:18:15.386352 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9991 ms
2025.07.02 09:18:15.497914 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9991 ms
2025.07.02 09:18:25.380493 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10013 ms
2025.07.02 09:18:25.386337 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10013 ms
2025.07.02 09:18:25.497904 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:18:35.380503 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:18:35.386339 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:18:35.497903 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10026 ms
2025.07.02 09:18:45.417851 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9989 ms
2025.07.02 09:18:45.423735 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9989 ms
2025.07.02 09:18:45.535220 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9988 ms
2025.07.02 09:18:52.712912 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 7184 ms
2025.07.02 09:18:52.714591 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 7303 ms
2025.07.02 09:18:52.742224 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7325 ms
2025.07.02 09:18:53.770591 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:18:53.770974 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:18:53.771193 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:18:53.772197 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:18:53.772720 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:18:54.236539 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:18:54.236984 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:18:54.237281 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:18:54.237568 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:18:54.237880 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:18:54.238198 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:18:54.238496 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:18:54.238855 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:18:54.239295 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:18:54.239476 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:18:54.239715 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:18:54.239910 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:18:54.240119 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 744
2025.07.02 09:18:54.240336 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 744
2025.07.02 09:18:54.240987 [ 1 ] {} <Information> RaftInstance: create snapshot idx 744 log_term 1
2025.07.02 09:18:54.241730 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:18:54.246320 [ 1 ] {} <Information> RaftInstance: snapshot idx 744 log_term 1 created, compact the log store if needed
2025.07.02 09:18:54.246638 [ 1 ] {} <Information> RaftInstance: create snapshot idx 744 log_term 1 done: 5377 us elapsed
2025.07.02 09:18:54.247261 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:18:54.247423 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:18:54.247294 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:18:54.247359 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:18:54.247400 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:18:54.247283 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:18:54.247565 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:18:54.247463 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:18:54.247668 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:18:54.247646 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:18:54.247707 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:18:54.247721 [ 59 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:18:54.247774 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:18:54.247432 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:18:54.247299 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:18:54.247818 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:18:54.251301 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:18:54.252473 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:18:54.253621 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:18:54.254773 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:18:54.254959 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:18:54.255449 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:19:04.872044 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:19:04.872522 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:19:04.872801 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:19:04.873798 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:19:04.875877 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:19:04.882607 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:19:04.885098 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:19:04.898527 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:19:04.899130 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:19:04.899319 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:19:04.899559 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:19:04.899736 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:19:04.899875 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:19:04.906556 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:19:04.909449 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:19:04.910020 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:19:04.911026 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:19:04.932327 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:19:04.933748 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:19:04.936209 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:19:04.936712 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:19:04.937089 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:19:04.937793 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:19:04.939158 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:19:04.939294 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 227.06 KiB to 30.45 MiB
2025.07.02 09:19:04.939739 [ 58 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:19:06.910993 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:19:06.912334 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:19:06.914553 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:19:06.919160 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:19:06.920134 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:19:06.920321 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:19:06.925376 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:19:06.926561 [ 57 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:19:06.927458 [ 57 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:19:06.931102 [ 57 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:19:06.932568 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:19:06.935728 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:19:06.954687 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:19:06.958460 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:19:06.959246 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:19:06.960385 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:19:06.960957 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:19:06.960886 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:19:08.765179 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:19:08.768921 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:19:08.804905 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:19:08.810141 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:19:08.909223 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:19:08.911590 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:19:18.769816 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9977 ms
2025.07.02 09:19:18.811140 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9992 ms
2025.07.02 09:19:18.912329 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9987 ms
2025.07.02 09:19:28.769849 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10023 ms
2025.07.02 09:19:28.811160 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10023 ms
2025.07.02 09:19:28.912304 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10023 ms
2025.07.02 09:19:38.829199 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10012 ms
2025.07.02 09:19:38.870449 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10012 ms
2025.07.02 09:19:38.971660 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10011 ms
2025.07.02 09:19:46.995952 [ 92 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:19:47.694733 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 8796 ms
2025.07.02 09:19:47.705333 [ 31 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 3 took 8706 ms
2025.07.02 09:19:47.705355 [ 29 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 1 took 8847 ms
2025.07.02 09:19:58.829273 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9600 ms
2025.07.02 09:19:58.870723 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9656 ms
2025.07.02 09:19:58.971708 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9746 ms
2025.07.02 09:20:08.849344 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10022 ms
2025.07.02 09:20:08.890661 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10022 ms
2025.07.02 09:20:08.991796 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10021 ms
2025.07.02 09:20:18.359000 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9493 ms
2025.07.02 09:20:18.890645 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9985 ms
2025.07.02 09:20:18.991621 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9985 ms
2025.07.02 09:20:21.576150 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 2729 ms
2025.07.02 09:20:22.266204 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 2675 ms
2025.07.02 09:20:22.513928 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 2438 ms
2025.07.02 09:20:24.840143 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 2329 ms
2025.07.02 09:20:26.038853 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 4470 ms
2025.07.02 09:20:27.349848 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 1312 ms
2025.07.02 09:20:28.849195 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 1502 ms
2025.07.02 09:20:28.890698 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 5035 ms
2025.07.02 09:20:28.991644 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 4160 ms
2025.07.02 09:20:38.879570 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10016 ms
2025.07.02 09:20:38.921035 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10016 ms
2025.07.02 09:20:39.022002 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10014 ms
2025.07.02 09:20:48.879557 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9965 ms
2025.07.02 09:20:48.921032 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9964 ms
2025.07.02 09:20:49.021986 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9966 ms
2025.07.02 09:20:58.879598 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10013 ms
2025.07.02 09:20:58.921045 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10013 ms
2025.07.02 09:20:59.021987 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10013 ms
2025.07.02 09:21:08.879788 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10023 ms
2025.07.02 09:21:08.921202 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10023 ms
2025.07.02 09:21:09.022189 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10022 ms
2025.07.02 09:21:18.879821 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10007 ms
2025.07.02 09:21:18.921151 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10007 ms
2025.07.02 09:21:19.022162 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10007 ms
2025.07.02 09:21:28.879825 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10023 ms
2025.07.02 09:21:28.921211 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10024 ms
2025.07.02 09:21:29.022183 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10024 ms
2025.07.02 09:21:38.940725 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10017 ms
2025.07.02 09:21:38.982017 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10016 ms
2025.07.02 09:21:39.083067 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10015 ms
2025.07.02 09:21:48.940582 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9979 ms
2025.07.02 09:21:48.981982 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9980 ms
2025.07.02 09:21:49.082993 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9981 ms
2025.07.02 09:21:58.940580 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10021 ms
2025.07.02 09:21:58.982052 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10021 ms
2025.07.02 09:21:59.082940 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10021 ms
2025.07.02 09:22:08.968028 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10019 ms
2025.07.02 09:22:09.009523 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10018 ms
2025.07.02 09:22:09.110489 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10017 ms
2025.07.02 09:22:18.968134 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9979 ms
2025.07.02 09:22:19.009547 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9979 ms
2025.07.02 09:22:19.110524 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9979 ms
2025.07.02 09:22:27.167337 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 8210 ms
2025.07.02 09:22:27.181865 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 8183 ms
2025.07.02 09:22:27.183719 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 8085 ms
2025.07.02 09:22:28.069494 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:22:28.070066 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:22:28.070317 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:22:28.071597 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:22:28.072208 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:22:28.215876 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:22:28.216224 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:22:28.216891 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:22:28.217132 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:22:28.217316 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:22:28.217970 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:22:28.218246 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:22:28.218499 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:22:28.218501 [ 58 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:22:28.219020 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:22:28.219251 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:22:28.219497 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:22:28.219709 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 511
2025.07.02 09:22:28.219901 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 511
2025.07.02 09:22:28.220159 [ 1 ] {} <Information> RaftInstance: create snapshot idx 511 log_term 1
2025.07.02 09:22:28.220424 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:22:28.224167 [ 1 ] {} <Information> RaftInstance: snapshot idx 511 log_term 1 created, compact the log store if needed
2025.07.02 09:22:28.224485 [ 1 ] {} <Information> RaftInstance: create snapshot idx 511 log_term 1 done: 4087 us elapsed
2025.07.02 09:22:28.225049 [ 41 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:22:28.225131 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:22:28.225146 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:22:28.225701 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:22:28.225215 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:22:28.224984 [ 44 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:22:28.225310 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:22:28.225020 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:22:28.225375 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:22:28.225690 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:22:28.225014 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:22:28.225566 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:22:28.225589 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:22:28.225683 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:22:28.225533 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:22:28.225478 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:22:28.226634 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:22:28.229587 [ 40 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:22:28.230434 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:22:28.231331 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:22:28.232609 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:22:28.233818 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:22:28.234080 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:22:28.234361 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:22:39.524688 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:22:39.527589 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:22:39.527894 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:22:39.528542 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:22:39.529307 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:22:39.532165 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:22:39.532734 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:22:39.549851 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:22:39.550507 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:22:39.550669 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:22:39.550799 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:22:39.550945 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:22:39.551120 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:22:39.556574 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:22:39.558466 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:22:39.559535 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:22:39.561361 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:22:39.576344 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:22:39.577127 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:22:39.577881 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:22:39.578418 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:22:39.578836 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:22:39.579489 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:22:39.580795 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:22:39.580951 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:22:39.586232 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 09:22:40.911074 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:22:40.911589 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:22:40.911803 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:22:40.913599 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:22:40.913898 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:22:40.914105 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:22:40.919035 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:22:40.919438 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:22:40.919848 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:22:40.920147 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:22:40.921050 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:22:40.923578 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:22:40.975122 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:22:40.981866 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:22:40.985191 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:22:40.985923 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:22:40.986142 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:22:40.986858 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:22:41.535308 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:22:41.538063 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:22:41.566388 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:22:41.571852 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:22:41.572140 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:22:41.576616 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:22:45.527811 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:22:45.530451 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:22:45.559690 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:22:45.571081 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:22:45.616900 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:22:45.624164 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:22:55.532666 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9728 ms
2025.07.02 09:22:55.573221 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9742 ms
2025.07.02 09:22:55.627023 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9792 ms
2025.07.02 09:23:05.532527 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10027 ms
2025.07.02 09:23:05.573245 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10027 ms
2025.07.02 09:23:05.625262 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:23:11.148953 [ 86 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:23:11.941240 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 6227 ms
2025.07.02 09:23:11.951081 [ 30 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 4 took 6326 ms
2025.07.02 09:23:11.951069 [ 31 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 5 took 6286 ms
2025.07.02 09:23:14.492766 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:23:14.493209 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:23:14.493462 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:23:14.493734 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:23:14.493980 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:23:14.494280 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:23:15.593811 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2467 ms
2025.07.02 09:23:15.634442 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2514 ms
2025.07.02 09:23:15.686459 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2576 ms
2025.07.02 09:23:17.885570 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2249 ms
2025.07.02 09:23:18.306521 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2710 ms
2025.07.02 09:23:18.487745 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1947 ms
2025.07.02 09:23:20.269909 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1893 ms
2025.07.02 09:23:20.338284 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1532 ms
2025.07.02 09:23:21.489440 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1219 ms
2025.07.02 09:23:22.588556 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 3809 ms
2025.07.02 09:23:25.593763 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3914 ms
2025.07.02 09:23:25.627068 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 5183 ms
2025.07.02 09:23:25.686425 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2577 ms
2025.07.02 09:23:35.593790 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:23:35.634407 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:23:35.686483 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10026 ms
2025.07.02 09:23:45.612539 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9979 ms
2025.07.02 09:23:45.653171 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9979 ms
2025.07.02 09:23:45.705225 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9979 ms
2025.07.02 09:23:55.612527 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 09:23:55.653173 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 09:23:55.705203 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10010 ms
2025.07.02 09:24:05.612556 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:24:05.653143 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:24:05.705215 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10026 ms
2025.07.02 09:24:15.633871 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9980 ms
2025.07.02 09:24:15.674412 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9980 ms
2025.07.02 09:24:15.726431 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9979 ms
2025.07.02 09:24:25.633729 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 09:24:25.674402 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 09:24:25.726478 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10010 ms
2025.07.02 09:24:35.633784 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:24:35.674405 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:24:35.726458 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:24:45.655913 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9998 ms
2025.07.02 09:24:45.696581 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9998 ms
2025.07.02 09:24:45.748628 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9998 ms
2025.07.02 09:24:55.655899 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10013 ms
2025.07.02 09:24:55.696580 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10014 ms
2025.07.02 09:24:55.748604 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:25:05.655953 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:25:05.696611 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:25:05.748635 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:25:15.700667 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9981 ms
2025.07.02 09:25:15.741301 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9977 ms
2025.07.02 09:25:15.793330 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9981 ms
2025.07.02 09:25:25.700644 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 09:25:25.741354 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 09:25:25.793361 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10006 ms
2025.07.02 09:25:35.700727 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:25:35.741379 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:25:35.793331 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:25:45.719196 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9982 ms
2025.07.02 09:25:45.759837 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9982 ms
2025.07.02 09:25:45.811897 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9982 ms
2025.07.02 09:25:55.719178 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 09:25:55.759820 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 09:25:55.811869 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10007 ms
2025.07.02 09:26:05.719300 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:26:05.760031 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:26:05.811889 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:26:15.740961 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9984 ms
2025.07.02 09:26:15.781582 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9984 ms
2025.07.02 09:26:15.833628 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9984 ms
2025.07.02 09:26:25.740942 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10007 ms
2025.07.02 09:26:25.781578 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10007 ms
2025.07.02 09:26:25.833645 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10007 ms
2025.07.02 09:26:28.283352 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2505 ms
2025.07.02 09:26:28.324274 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2587 ms
2025.07.02 09:26:28.353349 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2523 ms
2025.07.02 09:26:29.280833 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:26:29.282358 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:26:29.283084 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:26:29.284881 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:26:29.285223 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:26:29.684941 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:26:29.685369 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:26:29.685678 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:26:29.685886 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:26:29.686624 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:26:29.687291 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:26:29.687505 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:26:29.687792 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:26:29.688143 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:26:29.688403 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:26:29.688699 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:26:29.688852 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:26:29.689241 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 999
2025.07.02 09:26:29.689419 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 999
2025.07.02 09:26:29.689990 [ 1 ] {} <Information> RaftInstance: create snapshot idx 999 log_term 1
2025.07.02 09:26:29.690903 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:26:29.696094 [ 1 ] {} <Information> RaftInstance: snapshot idx 999 log_term 1 created, compact the log store if needed
2025.07.02 09:26:29.696429 [ 1 ] {} <Information> RaftInstance: create snapshot idx 999 log_term 1 done: 6081 us elapsed
2025.07.02 09:26:29.697196 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:26:29.697400 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:26:29.697501 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:26:29.698129 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:26:29.697747 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:26:29.697296 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:26:29.697719 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:26:29.697848 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:26:29.697942 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:26:29.698038 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:26:29.698247 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:26:29.697578 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:26:29.698377 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:26:29.698539 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:26:29.698609 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:26:29.698707 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:26:29.702075 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:26:29.702933 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:26:29.704268 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:26:29.705386 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:26:29.705630 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:26:29.706029 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:26:40.323326 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:26:40.324962 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:26:40.330201 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:26:40.331167 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:26:40.331787 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:26:40.340629 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:26:40.341827 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:26:40.357292 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:26:40.358271 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:26:40.358743 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:26:40.359219 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:26:40.359390 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:26:40.359707 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:26:40.369736 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:26:40.373704 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:26:40.376609 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:26:40.377709 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:26:40.387848 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:26:40.388526 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:26:40.388934 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:26:40.390082 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:26:40.391627 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:26:40.392233 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:26:40.393663 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:26:40.393961 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:26:40.398443 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 09:26:41.991194 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:26:41.992275 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:26:41.992619 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:26:41.997545 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:26:41.998238 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:26:41.998739 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:26:42.005436 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:26:42.006522 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:26:42.006992 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:26:42.007352 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:26:42.008643 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:26:42.009642 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:26:42.024793 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:26:42.025680 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:26:42.025979 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:26:42.026915 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:26:42.027248 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:26:42.027239 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:26:42.327784 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:26:42.331222 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:26:42.339803 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:26:42.343584 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:26:42.405414 [ 34 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:26:42.407797 [ 34 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:26:50.675041 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:26:50.679829 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:26:50.685349 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:26:50.687981 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:26:50.792030 [ 34 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:26:50.805147 [ 34 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:27:00.681394 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9744 ms
2025.07.02 09:27:00.689121 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9730 ms
2025.07.02 09:27:00.808288 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9805 ms
2025.07.02 09:27:10.702264 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9985 ms
2025.07.02 09:27:10.709911 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9991 ms
2025.07.02 09:27:10.829120 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10005 ms
2025.07.02 09:27:15.099193 [ 85 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:27:15.874791 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 5140 ms
2025.07.02 09:27:15.917670 [ 34 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 6 took 5062 ms
2025.07.02 09:27:15.917698 [ 33 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 5 took 5175 ms
2025.07.02 09:27:19.052178 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:27:19.052625 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:27:19.052879 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:27:19.053204 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:27:19.053560 [ 62 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:27:19.053903 [ 62 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:27:20.702210 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3491 ms
2025.07.02 09:27:20.709877 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3441 ms
2025.07.02 09:27:20.829061 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 3585 ms
2025.07.02 09:27:24.146705 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3439 ms
2025.07.02 09:27:26.083276 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3613 ms
2025.07.02 09:27:26.884464 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 6064 ms
2025.07.02 09:27:27.330873 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2489 ms
2025.07.02 09:27:29.657630 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2261 ms
2025.07.02 09:27:29.862310 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2536 ms
2025.07.02 09:27:29.973643 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3227 ms
2025.07.02 09:27:40.727217 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:27:40.734855 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:27:40.854034 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10022 ms
2025.07.02 09:27:50.727223 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9992 ms
2025.07.02 09:27:50.734897 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9993 ms
2025.07.02 09:27:50.853960 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9993 ms
2025.07.02 09:28:00.727195 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 09:28:00.734851 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 09:28:00.854003 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 09:28:10.763873 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9977 ms
2025.07.02 09:28:10.771584 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9983 ms
2025.07.02 09:28:10.890712 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10003 ms
2025.07.02 09:28:20.763829 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9940 ms
2025.07.02 09:28:20.771507 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9946 ms
2025.07.02 09:28:20.890690 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9964 ms
2025.07.02 09:28:30.763961 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10014 ms
2025.07.02 09:28:30.771577 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10014 ms
2025.07.02 09:28:30.890784 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 09:28:40.763438 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9965 ms
2025.07.02 09:28:40.770864 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9970 ms
2025.07.02 09:28:40.890102 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 09:28:50.763172 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9984 ms
2025.07.02 09:28:50.770757 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9985 ms
2025.07.02 09:28:50.889992 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9984 ms
2025.07.02 09:29:00.763104 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10019 ms
2025.07.02 09:29:00.770788 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10019 ms
2025.07.02 09:29:00.889996 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10019 ms
2025.07.02 09:29:10.788064 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10016 ms
2025.07.02 09:29:10.795696 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10016 ms
2025.07.02 09:29:10.914906 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 09:29:20.788028 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9998 ms
2025.07.02 09:29:20.795718 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9998 ms
2025.07.02 09:29:20.914890 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9998 ms
2025.07.02 09:29:30.788391 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 09:29:30.795845 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 09:29:30.914896 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 09:29:40.832732 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10004 ms
2025.07.02 09:29:40.840456 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10007 ms
2025.07.02 09:29:40.959545 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10006 ms
2025.07.02 09:29:50.832713 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9964 ms
2025.07.02 09:29:50.840435 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9969 ms
2025.07.02 09:29:50.959590 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9979 ms
2025.07.02 09:30:00.832746 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10027 ms
2025.07.02 09:30:00.840405 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10027 ms
2025.07.02 09:30:00.959603 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10028 ms
2025.07.02 09:30:10.854282 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10013 ms
2025.07.02 09:30:10.862458 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10014 ms
2025.07.02 09:30:10.981121 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10012 ms
2025.07.02 09:30:19.499907 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 8627 ms
2025.07.02 09:30:19.518355 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 8644 ms
2025.07.02 09:30:19.527154 [ 34 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 8535 ms
2025.07.02 09:30:20.504509 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:30:20.507326 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:30:20.507718 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:30:20.509922 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:30:20.510431 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:30:20.739921 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:30:20.740355 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:30:20.740602 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:30:20.740869 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:30:20.741576 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:30:20.742147 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:30:20.742429 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:30:20.742716 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:30:20.742763 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:30:20.743287 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:30:20.743534 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:30:20.744108 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:30:20.744343 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 995
2025.07.02 09:30:20.744581 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 995
2025.07.02 09:30:20.745148 [ 1 ] {} <Information> RaftInstance: create snapshot idx 995 log_term 1
2025.07.02 09:30:20.745785 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:30:20.750984 [ 1 ] {} <Information> RaftInstance: snapshot idx 995 log_term 1 created, compact the log store if needed
2025.07.02 09:30:20.751334 [ 1 ] {} <Information> RaftInstance: create snapshot idx 995 log_term 1 done: 5988 us elapsed
2025.07.02 09:30:20.752298 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:30:20.752594 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:30:20.752844 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:30:20.752366 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:30:20.752452 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:30:20.752420 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:30:20.752511 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:30:20.752532 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:30:20.752399 [ 48 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:30:20.752598 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:30:20.752646 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:30:20.752315 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:30:20.752634 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:30:20.752541 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:30:20.752310 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:30:20.752884 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:30:20.754426 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:30:20.756378 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:30:20.757113 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:30:20.758970 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:30:20.760218 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:30:20.761662 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:30:20.761915 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:30:20.762360 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:30:32.009827 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:30:32.012032 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:30:32.012828 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:30:32.013871 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:30:32.015801 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:30:32.024363 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:30:32.025650 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:30:32.033128 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:30:32.033687 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:30:32.035398 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:30:32.035671 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:30:32.035862 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:30:32.036076 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:30:32.043687 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:30:32.051474 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:30:32.052275 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:30:32.053711 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:30:32.071469 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:30:32.072559 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:30:32.073140 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:30:32.073393 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:30:32.074255 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:30:32.074807 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:30:32.075189 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:30:32.081457 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:30:32.081990 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 09:30:33.425486 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:30:33.425974 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:30:33.426216 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:30:33.430894 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:30:33.431180 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:30:33.433456 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:30:33.435832 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:30:33.436141 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:30:33.436376 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:30:33.441453 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:30:33.442924 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:30:33.443514 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:30:33.471125 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:30:33.473644 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:30:33.474224 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:30:33.476050 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:30:33.478269 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:30:33.476946 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:30:33.809650 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:30:33.815437 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:30:33.848560 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:30:33.854970 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:30:33.855754 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:30:33.862856 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:30:37.515605 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:30:37.521843 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:30:37.577658 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:30:37.582916 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:30:37.754757 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:30:37.757020 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:30:47.556090 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9260 ms
2025.07.02 09:30:47.617647 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9093 ms
2025.07.02 09:30:47.791445 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9737 ms
2025.07.02 09:30:57.556057 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10008 ms
2025.07.02 09:30:57.617544 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10008 ms
2025.07.02 09:30:57.791431 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10009 ms
2025.07.02 09:31:03.651608 [ 86 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:31:04.429030 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 6890 ms
2025.07.02 09:31:04.440434 [ 32 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 6 took 6665 ms
2025.07.02 09:31:04.440442 [ 30 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 5 took 6840 ms
2025.07.02 09:31:05.984238 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:31:05.984728 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:31:05.984965 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:31:05.985173 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:31:05.985788 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:31:05.986060 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:31:07.556063 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1937 ms
2025.07.02 09:31:07.617616 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1981 ms
2025.07.02 09:31:07.791362 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2157 ms
2025.07.02 09:31:08.673136 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1111 ms
2025.07.02 09:31:08.884979 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1262 ms
2025.07.02 09:31:09.265344 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1467 ms
2025.07.02 09:31:12.612021 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3919 ms
2025.07.02 09:31:14.095035 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 4806 ms
2025.07.02 09:31:16.300788 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 5809 ms
2025.07.02 09:31:16.884991 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3909 ms
2025.07.02 09:31:17.798423 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2546 ms
2025.07.02 09:31:27.563164 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9950 ms
2025.07.02 09:31:27.624652 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10013 ms
2025.07.02 09:31:27.798460 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:31:37.563123 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10028 ms
2025.07.02 09:31:37.624595 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 09:31:37.798399 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10028 ms
2025.07.02 09:31:47.589528 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10010 ms
2025.07.02 09:31:47.651034 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 09:31:47.824847 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10010 ms
2025.07.02 09:31:57.589674 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10021 ms
2025.07.02 09:31:57.651049 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10021 ms
2025.07.02 09:31:57.824874 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10021 ms
2025.07.02 09:32:07.589655 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9987 ms
2025.07.02 09:32:07.651029 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9988 ms
2025.07.02 09:32:07.824819 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9988 ms
2025.07.02 09:32:17.616836 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9948 ms
2025.07.02 09:32:17.678301 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9947 ms
2025.07.02 09:32:17.852125 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9947 ms
2025.07.02 09:32:27.616862 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10003 ms
2025.07.02 09:32:27.678305 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10003 ms
2025.07.02 09:32:27.852141 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10005 ms
2025.07.02 09:32:37.616830 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10031 ms
2025.07.02 09:32:37.678298 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10032 ms
2025.07.02 09:32:37.852188 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10032 ms
2025.07.02 09:32:47.615987 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9989 ms
2025.07.02 09:32:47.677460 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9989 ms
2025.07.02 09:32:47.851341 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9988 ms
2025.07.02 09:32:57.616033 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:32:57.677481 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:32:57.851418 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10024 ms
2025.07.02 09:33:07.616017 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10020 ms
2025.07.02 09:33:07.677506 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10019 ms
2025.07.02 09:33:07.851391 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10018 ms
2025.07.02 09:33:17.641966 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9959 ms
2025.07.02 09:33:17.703465 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9959 ms
2025.07.02 09:33:17.877321 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9960 ms
2025.07.02 09:33:27.641972 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10032 ms
2025.07.02 09:33:27.703466 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10031 ms
2025.07.02 09:33:27.877313 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10033 ms
2025.07.02 09:33:37.642009 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10028 ms
2025.07.02 09:33:37.703483 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10027 ms
2025.07.02 09:33:37.877339 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10026 ms
2025.07.02 09:33:47.673143 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9962 ms
2025.07.02 09:33:47.734630 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9962 ms
2025.07.02 09:33:47.908478 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9962 ms
2025.07.02 09:33:57.673176 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10024 ms
2025.07.02 09:33:57.734732 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:33:57.908445 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:34:07.673156 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10036 ms
2025.07.02 09:34:07.734641 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10036 ms
2025.07.02 09:34:07.908492 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10036 ms
2025.07.02 09:34:10.234006 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2285 ms
2025.07.02 09:34:10.288531 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2513 ms
2025.07.02 09:34:10.301117 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2588 ms
2025.07.02 09:34:11.197410 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:34:11.198018 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:34:11.198756 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:34:11.200400 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:34:11.200668 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:34:11.671017 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:34:11.671452 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:34:11.671733 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:34:11.672046 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:34:11.672328 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:34:11.672567 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:34:11.672824 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:34:11.673168 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:34:11.673543 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:34:11.673819 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:34:11.674126 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:34:11.674407 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:34:11.674706 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 992
2025.07.02 09:34:11.674997 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 992
2025.07.02 09:34:11.675223 [ 1 ] {} <Information> RaftInstance: create snapshot idx 992 log_term 1
2025.07.02 09:34:11.675530 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:34:11.680529 [ 1 ] {} <Information> RaftInstance: snapshot idx 992 log_term 1 created, compact the log store if needed
2025.07.02 09:34:11.680827 [ 1 ] {} <Information> RaftInstance: create snapshot idx 992 log_term 1 done: 5379 us elapsed
2025.07.02 09:34:11.681301 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:34:11.681539 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:34:11.681806 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:34:11.681433 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:34:11.681218 [ 47 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:34:11.681462 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:34:11.681474 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:34:11.681485 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:34:11.681386 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:34:11.681606 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:34:11.681698 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:34:11.681762 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:34:11.681735 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:34:11.681437 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:34:11.681796 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:34:11.681923 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:34:11.682575 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:34:11.685198 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:34:11.686012 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:34:11.686967 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:34:11.688159 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:34:11.689228 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:34:11.689509 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:34:11.689863 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:34:22.049431 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:34:22.050255 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:34:22.051154 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:34:22.051959 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:34:22.052583 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:34:22.056225 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:34:22.056694 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:34:22.069257 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:34:22.070197 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:34:22.070383 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:34:22.070596 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:34:22.070864 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:34:22.071038 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:34:22.079440 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:34:22.083274 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:34:22.086390 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:34:22.087290 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:34:22.107210 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:34:22.110008 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:34:22.111343 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:34:22.111692 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:34:22.111937 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:34:22.112224 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:34:22.116037 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:34:22.116107 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:34:22.116358 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 09:34:23.992711 [ 43 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:34:23.993413 [ 43 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:34:23.993670 [ 43 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:34:23.996126 [ 43 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:34:23.996420 [ 43 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:34:23.996669 [ 43 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:34:23.998539 [ 43 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:34:23.998947 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:34:23.999333 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:34:23.999589 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:34:24.000574 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:34:24.000946 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:34:24.014913 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:34:24.015644 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:34:24.016035 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:34:24.016690 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:34:24.016921 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:34:24.016942 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:34:24.025087 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:34:24.028110 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:34:24.101702 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:34:24.107317 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:34:24.142225 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:34:24.144922 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:34:32.532576 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:34:32.534595 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:34:32.543827 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:34:32.546063 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:34:32.558789 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:34:32.563478 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:34:42.541583 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9757 ms
2025.07.02 09:34:42.553083 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9769 ms
2025.07.02 09:34:42.570908 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9779 ms
2025.07.02 09:34:52.541579 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9990 ms
2025.07.02 09:34:52.552965 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9990 ms
2025.07.02 09:34:52.570738 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9990 ms
2025.07.02 09:35:00.518656 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:35:00.519356 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:35:00.519855 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:35:00.520105 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:35:00.520399 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:35:00.520597 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:35:02.541555 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10025 ms
2025.07.02 09:35:02.552945 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10025 ms
2025.07.02 09:35:02.570753 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10025 ms
2025.07.02 09:35:03.324490 [ 92 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:35:04.039755 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1500 ms
2025.07.02 09:35:04.048951 [ 30 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 5 took 1499 ms
2025.07.02 09:35:04.048955 [ 33 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 6 took 1480 ms
2025.07.02 09:35:07.240482 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1641 ms
2025.07.02 09:35:08.825251 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1557 ms
2025.07.02 09:35:09.271357 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 2480 ms
2025.07.02 09:35:09.768823 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 4396 ms
2025.07.02 09:35:10.721926 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1440 ms
2025.07.02 09:35:12.530731 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3686 ms
2025.07.02 09:35:12.571893 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1132 ms
2025.07.02 09:35:12.601079 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1014 ms
2025.07.02 09:35:22.571925 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9995 ms
2025.07.02 09:35:22.583285 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9921 ms
2025.07.02 09:35:22.601058 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9996 ms
2025.07.02 09:35:32.571903 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:35:32.583283 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10027 ms
2025.07.02 09:35:32.601135 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10027 ms
2025.07.02 09:35:42.602974 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10007 ms
2025.07.02 09:35:42.614400 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10007 ms
2025.07.02 09:35:42.632164 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10006 ms
2025.07.02 09:35:52.602950 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10003 ms
2025.07.02 09:35:52.614378 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10002 ms
2025.07.02 09:35:52.632150 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10003 ms
2025.07.02 09:36:02.602958 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10026 ms
2025.07.02 09:36:02.614335 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10026 ms
2025.07.02 09:36:02.632235 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10026 ms
2025.07.02 09:36:12.644606 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9998 ms
2025.07.02 09:36:12.655911 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9998 ms
2025.07.02 09:36:12.673817 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9999 ms
2025.07.02 09:36:22.644502 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10003 ms
2025.07.02 09:36:22.655876 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10004 ms
2025.07.02 09:36:22.673735 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10003 ms
2025.07.02 09:36:30.545947 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 7890 ms
2025.07.02 09:36:30.553280 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7916 ms
2025.07.02 09:36:30.557985 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 7931 ms
2025.07.02 09:36:31.596778 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:36:31.597797 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:36:31.598048 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:36:31.599306 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:36:31.599681 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:36:31.645339 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:36:31.645709 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:36:31.645980 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:36:31.646292 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:36:31.646562 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:36:31.646986 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:36:31.647289 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:36:31.647942 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:36:31.648120 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:36:31.648757 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:36:31.649007 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:36:31.649421 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:36:31.649636 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 969
2025.07.02 09:36:31.649879 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 969
2025.07.02 09:36:31.650081 [ 1 ] {} <Information> RaftInstance: create snapshot idx 969 log_term 1
2025.07.02 09:36:31.650939 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:36:31.657997 [ 1 ] {} <Information> RaftInstance: snapshot idx 969 log_term 1 created, compact the log store if needed
2025.07.02 09:36:31.658332 [ 1 ] {} <Information> RaftInstance: create snapshot idx 969 log_term 1 done: 8048 us elapsed
2025.07.02 09:36:31.658638 [ 50 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:36:31.659041 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:36:31.659207 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:36:31.659089 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:36:31.659179 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:36:31.659266 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:36:31.659314 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:36:31.659336 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:36:31.659358 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:36:31.659369 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:36:31.659016 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:36:31.659483 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:36:31.659112 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:36:31.659555 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:36:31.659598 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:36:31.659152 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:36:31.659855 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:36:31.664820 [ 42 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:36:31.665806 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:36:31.666546 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:36:31.667781 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:36:31.669975 [ 1 ] {} <Information> Application: Background threads finished in 2 ms
2025.07.02 09:36:31.670346 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:36:31.671590 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:36:42.465120 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:36:42.465533 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:36:42.466040 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:36:42.466942 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:36:42.467347 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:36:42.471652 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:36:42.472226 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:36:42.487444 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:36:42.488025 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:36:42.488333 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:36:42.488603 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:36:42.491935 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:36:42.494997 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:36:42.515226 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:36:42.521308 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:36:42.522922 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:36:42.523354 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:36:42.525620 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 97.31 KiB to 28.13 MiB
2025.07.02 09:36:42.545775 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:36:42.546567 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:36:42.547398 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:36:42.547682 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:36:42.547901 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:36:42.548167 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:36:42.548690 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:36:42.548898 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:36:44.436669 [ 45 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:36:44.438052 [ 45 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:36:44.438507 [ 45 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:36:44.440571 [ 45 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:36:44.440834 [ 45 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:36:44.441048 [ 45 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:36:44.444059 [ 45 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:36:44.444492 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:36:44.444744 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:36:44.445036 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:36:44.445793 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:36:44.446652 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:36:44.461061 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:36:44.462848 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:36:44.464647 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:36:44.466691 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:36:44.467681 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:36:44.468344 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:36:44.467912 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:36:44.470311 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:36:44.495980 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:36:44.498955 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:36:45.767519 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:36:45.769630 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:36:49.257448 [ 32 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 3 took 3414 ms
2025.07.02 09:36:51.586399 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:36:51.590120 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:36:51.703829 [ 31 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:36:51.705808 [ 31 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:36:52.776522 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1015 ms
2025.07.02 09:36:57.341978 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1100 ms
2025.07.02 09:36:59.963185 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 2624 ms
2025.07.02 09:37:01.591088 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9895 ms
2025.07.02 09:37:01.706872 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9913 ms
2025.07.02 09:37:05.013626 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 5063 ms
2025.07.02 09:37:11.592439 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 09:37:11.708224 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10005 ms
2025.07.02 09:37:12.230811 [ 80 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:37:12.953782 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1354 ms
2025.07.02 09:37:12.963825 [ 32 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 3 took 7180 ms
2025.07.02 09:37:12.963825 [ 31 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 5 took 1248 ms
2025.07.02 09:37:20.183260 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1079 ms
2025.07.02 09:37:20.958397 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:37:20.958857 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:37:20.959145 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:37:20.959477 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:37:21.592435 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 7639 ms
2025.07.02 09:37:21.708298 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7743 ms
2025.07.02 09:37:23.582672 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1990 ms
2025.07.02 09:37:24.139251 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1682 ms
2025.07.02 09:37:24.924799 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1265 ms
2025.07.02 09:37:25.081612 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2938 ms
2025.07.02 09:37:27.050972 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1280 ms
2025.07.02 09:37:27.140982 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 2220 ms
2025.07.02 09:37:28.110072 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2642 ms
2025.07.02 09:37:28.616921 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1127 ms
2025.07.02 09:37:30.525884 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1676 ms
2025.07.02 09:37:31.120970 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3989 ms
2025.07.02 09:37:31.708265 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3607 ms
2025.07.02 09:37:35.682030 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 5171 ms
2025.07.02 09:37:37.366322 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1597 ms
2025.07.02 09:37:41.620073 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10000 ms
2025.07.02 09:37:41.735827 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9999 ms
2025.07.02 09:37:45.799353 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 8300 ms
2025.07.02 09:37:51.620039 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9978 ms
2025.07.02 09:37:51.735878 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9979 ms
2025.07.02 09:37:52.980882 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 7178 ms
2025.07.02 09:37:54.396141 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1402 ms
2025.07.02 09:38:01.620072 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10020 ms
2025.07.02 09:38:01.735864 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10020 ms
2025.07.02 09:38:03.636068 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 7856 ms
2025.07.02 09:38:05.799318 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 2168 ms
2025.07.02 09:38:11.626489 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 09:38:11.742369 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 09:38:15.805752 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9964 ms
2025.07.02 09:38:21.626489 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9989 ms
2025.07.02 09:38:21.742311 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9990 ms
2025.07.02 09:38:25.805789 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10009 ms
2025.07.02 09:38:31.626524 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:38:31.742289 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 09:38:35.805790 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10025 ms
2025.07.02 09:38:41.651863 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10009 ms
2025.07.02 09:38:41.767599 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10010 ms
2025.07.02 09:38:45.831112 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9991 ms
2025.07.02 09:38:50.003889 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 4173 ms
2025.07.02 09:38:51.651779 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9996 ms
2025.07.02 09:38:51.767574 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9997 ms
2025.07.02 09:38:52.003408 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1001 ms
2025.07.02 09:38:53.003694 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1001 ms
2025.07.02 09:38:54.003538 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1001 ms
2025.07.02 09:38:55.003650 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1001 ms
2025.07.02 09:38:57.003622 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:38:58.003762 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:38:59.003737 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1001 ms
2025.07.02 09:39:00.003540 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1001 ms
2025.07.02 09:39:01.003607 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:39:01.651713 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10024 ms
2025.07.02 09:39:01.767539 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:39:02.003464 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:39:03.003524 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:39:04.003463 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:39:05.003485 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:39:07.003456 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:39:08.003921 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 1002 ms
2025.07.02 09:39:11.691179 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 09:39:11.807020 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10005 ms
2025.07.02 09:39:15.870487 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 7375 ms
2025.07.02 09:39:21.691154 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9990 ms
2025.07.02 09:39:21.807029 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9991 ms
2025.07.02 09:39:25.870494 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10010 ms
2025.07.02 09:39:31.691236 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10024 ms
2025.07.02 09:39:31.807030 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10024 ms
2025.07.02 09:39:35.870470 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10029 ms
2025.07.02 09:39:41.718429 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10007 ms
2025.07.02 09:39:41.834374 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10006 ms
2025.07.02 09:39:45.897774 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9984 ms
2025.07.02 09:39:51.718505 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9991 ms
2025.07.02 09:39:51.834357 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9991 ms
2025.07.02 09:39:55.897799 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10010 ms
2025.07.02 09:40:01.718508 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 09:40:01.834394 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 09:40:05.897736 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10027 ms
2025.07.02 09:40:11.747435 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 09:40:11.863247 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10005 ms
2025.07.02 09:40:15.926735 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9984 ms
2025.07.02 09:40:21.747424 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9991 ms
2025.07.02 09:40:21.863261 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9992 ms
2025.07.02 09:40:25.926733 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10009 ms
2025.07.02 09:40:31.747420 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10022 ms
2025.07.02 09:40:31.863271 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10022 ms
2025.07.02 09:40:35.926758 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10027 ms
2025.07.02 09:40:40.484904 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 4516 ms
2025.07.02 09:40:40.485320 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 8722 ms
2025.07.02 09:40:40.512966 [ 31 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 8633 ms
2025.07.02 09:40:41.446149 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:40:41.450527 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:40:41.450977 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:40:41.452295 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:40:41.452975 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:40:41.689320 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:40:41.689788 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:40:41.690094 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:40:41.690350 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:40:41.690679 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:40:41.690906 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:40:41.691134 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:40:41.691481 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:40:41.691535 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:40:41.692081 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:40:41.692366 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:40:41.692653 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:40:41.692892 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 839
2025.07.02 09:40:41.693140 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 839
2025.07.02 09:40:41.693322 [ 1 ] {} <Information> RaftInstance: create snapshot idx 839 log_term 1
2025.07.02 09:40:41.693613 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:40:41.697383 [ 1 ] {} <Information> RaftInstance: snapshot idx 839 log_term 1 created, compact the log store if needed
2025.07.02 09:40:41.697698 [ 1 ] {} <Information> RaftInstance: create snapshot idx 839 log_term 1 done: 4139 us elapsed
2025.07.02 09:40:41.698346 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:40:41.698439 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:40:41.698750 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:40:41.698870 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:40:41.699270 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:40:41.698155 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:40:41.698641 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:40:41.698811 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:40:41.698479 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:40:41.698488 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:40:41.699034 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:40:41.699104 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:40:41.699126 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:40:41.699166 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:40:41.698526 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:40:41.699267 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:40:41.699920 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:40:41.702799 [ 42 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:40:41.703674 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:40:41.704611 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:40:41.705802 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:40:41.707060 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:40:41.707334 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:40:41.707668 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:40:52.328167 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:40:52.329338 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:40:52.329893 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:40:52.330830 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:40:52.331349 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:40:52.334274 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:40:52.334641 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:40:52.344031 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:40:52.344554 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:40:52.344975 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:40:52.345139 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:40:52.345439 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:40:52.345656 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:40:52.351710 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:40:52.353775 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:40:52.354357 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:40:52.354937 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:40:52.368759 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:40:52.369224 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:40:52.369534 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:40:52.370172 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:40:52.370540 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:40:52.370766 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:40:52.371220 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:40:52.371712 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:40:52.388854 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.80 MiB
2025.07.02 09:40:53.677158 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:40:53.680419 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:40:53.680722 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:40:53.683760 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:40:53.684123 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:40:53.684382 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:40:53.686596 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:40:53.686994 [ 58 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:40:53.687305 [ 58 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:40:53.687563 [ 58 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:40:53.688305 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:40:53.689027 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:40:53.716173 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:40:53.720409 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:40:53.722246 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:40:53.723352 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:40:53.723869 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:40:53.723957 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:40:53.918077 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:40:53.920942 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:40:53.967326 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:40:53.972567 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:40:54.003897 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:40:54.007812 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:40:57.682403 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:40:57.687111 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 4
2025.07.02 09:40:57.766683 [ 29 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:40:57.771237 [ 29 ] {} <Information> KeeperTCPHandler: Received session ID 5
2025.07.02 09:40:57.846694 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:40:57.853595 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 6
2025.07.02 09:41:07.689585 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9699 ms
2025.07.02 09:41:07.772460 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9759 ms
2025.07.02 09:41:07.854584 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9799 ms
2025.07.02 09:41:17.716701 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9985 ms
2025.07.02 09:41:17.799463 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9985 ms
2025.07.02 09:41:17.881718 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9985 ms
2025.07.02 09:41:24.872881 [ 86 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:41:25.568223 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 7780 ms
2025.07.02 09:41:25.577735 [ 32 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 6 took 7708 ms
2025.07.02 09:41:25.577753 [ 30 ] {} <Information> KeeperTCPHandler: Waiting for response to be ready for session 4 took 7849 ms
2025.07.02 09:41:26.227760 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 1, will try to close it
2025.07.02 09:41:26.228219 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:41:26.228447 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 3, will try to close it
2025.07.02 09:41:26.228779 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:41:26.229061 [ 61 ] {} <Information> KeeperDispatcher: Found dead session 2, will try to close it
2025.07.02 09:41:26.229336 [ 61 ] {} <Information> KeeperDispatcher: Dead session close request pushed
2025.07.02 09:41:27.716778 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1133 ms
2025.07.02 09:41:27.799538 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1231 ms
2025.07.02 09:41:27.881832 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1297 ms
2025.07.02 09:41:29.417405 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1538 ms
2025.07.02 09:41:30.995121 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 3286 ms
2025.07.02 09:41:31.440072 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 3650 ms
2025.07.02 09:41:35.384426 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 2843 ms
2025.07.02 09:41:35.491711 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 6092 ms
2025.07.02 09:41:35.890429 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 4535 ms
2025.07.02 09:41:37.299351 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 1799 ms
2025.07.02 09:41:37.716729 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1830 ms
2025.07.02 09:41:37.799522 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 1682 ms
2025.07.02 09:41:47.757724 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9977 ms
2025.07.02 09:41:47.840580 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9977 ms
2025.07.02 09:41:47.922842 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9976 ms
2025.07.02 09:41:57.757801 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9996 ms
2025.07.02 09:41:57.840554 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10015 ms
2025.07.02 09:41:57.922848 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10015 ms
2025.07.02 09:42:07.757811 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10028 ms
2025.07.02 09:42:07.840548 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10029 ms
2025.07.02 09:42:07.922850 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10029 ms
2025.07.02 09:42:17.786165 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9991 ms
2025.07.02 09:42:17.868871 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9990 ms
2025.07.02 09:42:17.951178 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9990 ms
2025.07.02 09:42:27.786180 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10006 ms
2025.07.02 09:42:27.868878 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10019 ms
2025.07.02 09:42:27.951194 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10019 ms
2025.07.02 09:42:37.786145 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10030 ms
2025.07.02 09:42:37.868857 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10030 ms
2025.07.02 09:42:37.951194 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10030 ms
2025.07.02 09:42:47.833469 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9981 ms
2025.07.02 09:42:47.916141 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9981 ms
2025.07.02 09:42:47.998464 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9981 ms
2025.07.02 09:42:57.833460 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10016 ms
2025.07.02 09:42:57.916197 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10016 ms
2025.07.02 09:42:57.998480 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:43:07.833444 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10029 ms
2025.07.02 09:43:07.916220 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 09:43:07.998487 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10026 ms
2025.07.02 09:43:17.866667 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10007 ms
2025.07.02 09:43:17.949363 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10007 ms
2025.07.02 09:43:18.031684 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10004 ms
2025.07.02 09:43:27.866656 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:43:27.949410 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:43:28.031712 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:43:37.866672 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10029 ms
2025.07.02 09:43:37.949395 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10029 ms
2025.07.02 09:43:38.031715 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10029 ms
2025.07.02 09:43:47.932588 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9984 ms
2025.07.02 09:43:48.015319 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9983 ms
2025.07.02 09:43:48.097655 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9983 ms
2025.07.02 09:43:57.932602 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10014 ms
2025.07.02 09:43:58.015289 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10014 ms
2025.07.02 09:43:58.097620 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10014 ms
2025.07.02 09:44:07.932609 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10028 ms
2025.07.02 09:44:08.015266 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 09:44:08.097606 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10028 ms
2025.07.02 09:44:17.965400 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9979 ms
2025.07.02 09:44:18.048133 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9979 ms
2025.07.02 09:44:18.130414 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9979 ms
2025.07.02 09:44:27.965335 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10012 ms
2025.07.02 09:44:28.048108 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10013 ms
2025.07.02 09:44:28.130323 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10013 ms
2025.07.02 09:44:37.965404 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10023 ms
2025.07.02 09:44:38.048064 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10023 ms
2025.07.02 09:44:38.130360 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10023 ms
2025.07.02 09:44:47.987773 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9969 ms
2025.07.02 09:44:48.070467 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9968 ms
2025.07.02 09:44:48.152783 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9968 ms
2025.07.02 09:44:57.987788 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10008 ms
2025.07.02 09:44:58.070482 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10008 ms
2025.07.02 09:44:58.152787 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10008 ms
2025.07.02 09:45:07.987753 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10028 ms
2025.07.02 09:45:08.070486 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10028 ms
2025.07.02 09:45:08.152794 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10028 ms
2025.07.02 09:45:17.999940 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 9991 ms
2025.07.02 09:45:18.082610 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 9991 ms
2025.07.02 09:45:18.164930 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 9991 ms
2025.07.02 09:45:27.999899 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10015 ms
2025.07.02 09:45:28.082646 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10017 ms
2025.07.02 09:45:28.164952 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10017 ms
2025.07.02 09:45:37.999940 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 10027 ms
2025.07.02 09:45:38.082639 [ 29 ] {} <Information> KeeperTCPHandler: Receiving request for session 5 took 10029 ms
2025.07.02 09:45:38.164930 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 6 took 10029 ms
2025.07.02 09:45:39.009043 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 4 took 1008 ms
2025.07.02 09:45:40.069024 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:45:40.070183 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:45:40.070528 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:45:40.072570 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:45:40.072933 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:45:40.113659 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:45:40.114121 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:45:40.114460 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:45:40.114763 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:45:40.115012 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:45:40.115236 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:45:40.115479 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:45:40.115766 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:45:40.115768 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:45:40.116265 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:45:40.116543 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:45:40.116801 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:45:40.117062 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1016
2025.07.02 09:45:40.117302 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1016
2025.07.02 09:45:40.117604 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1016 log_term 1
2025.07.02 09:45:40.117869 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:45:40.121821 [ 1 ] {} <Information> RaftInstance: snapshot idx 1016 log_term 1 created, compact the log store if needed
2025.07.02 09:45:40.122228 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1016 log_term 1 done: 4385 us elapsed
2025.07.02 09:45:40.122688 [ 46 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:45:40.123341 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:45:40.123077 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:45:40.123151 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:45:40.123177 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:45:40.123217 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:45:40.123267 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:45:40.123324 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:45:40.123427 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:45:40.123039 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:45:40.123396 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:45:40.123449 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:45:40.123481 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:45:40.123507 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:45:40.123541 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:45:40.123590 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:45:40.123627 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:45:40.127447 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:45:40.128387 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:45:40.129365 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:45:40.130683 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:45:40.132008 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:45:40.132313 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:45:40.132609 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:45:51.147605 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:45:51.149166 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:45:51.149404 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:45:51.149982 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:45:51.150537 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:45:51.162949 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:45:51.163326 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:45:51.169574 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:45:51.175706 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:45:51.176067 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:45:51.176296 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:45:51.176677 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:45:51.177828 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:45:51.200441 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:45:51.202736 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:45:51.203836 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:45:51.204263 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:45:51.214992 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:45:51.215512 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:45:51.215784 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:45:51.216019 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:45:51.216893 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:45:51.217400 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:45:51.217733 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:45:51.217901 [ 60 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:45:51.226076 [ 28 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 243.28 KiB to 30.81 MiB
2025.07.02 09:45:52.956400 [ 44 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:45:52.956995 [ 44 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:45:52.957226 [ 44 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:45:52.959702 [ 44 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:45:52.959981 [ 44 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:45:52.960224 [ 44 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:45:52.967540 [ 44 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:45:52.967992 [ 59 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:45:52.968205 [ 59 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:45:52.968533 [ 59 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:45:52.969270 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:45:52.969560 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:45:52.980678 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:45:52.981444 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:45:52.981737 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:45:52.982632 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:45:52.982861 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:45:52.982897 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:46:21.081451 [ 29 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:48:04.051115 [ 27 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:48:04.051989 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:48:04.052232 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:48:04.053478 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:48:04.054427 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:48:04.151879 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:48:04.152342 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:48:04.152600 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:48:04.152837 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:48:04.153119 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:48:04.153374 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:48:04.153613 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:48:04.153846 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:48:04.153849 [ 60 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:48:04.154405 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:48:04.154672 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:48:04.154919 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:48:04.155170 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 1
2025.07.02 09:48:04.155466 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 1
2025.07.02 09:48:04.155686 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1
2025.07.02 09:48:04.155948 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:48:04.159387 [ 1 ] {} <Information> RaftInstance: snapshot idx 1 log_term 1 created, compact the log store if needed
2025.07.02 09:48:04.159748 [ 1 ] {} <Information> RaftInstance: create snapshot idx 1 log_term 1 done: 3840 us elapsed
2025.07.02 09:48:04.160379 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15
2025.07.02 09:48:04.160767 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:48:04.160427 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:48:04.160483 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:48:04.160204 [ 47 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:48:04.160486 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:48:04.160653 [ 41 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:48:04.160596 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:48:04.160594 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:48:04.160471 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:48:04.160384 [ 58 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:48:04.160764 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:48:04.160910 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:48:04.161071 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:48:04.161021 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:48:04.161120 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:48:04.161744 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:48:04.165096 [ 40 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:48:04.166128 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:48:04.166876 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:48:04.167745 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:48:04.169823 [ 1 ] {} <Information> Application: Background threads finished in 2 ms
2025.07.02 09:48:04.170165 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:48:04.170410 [ 27 ] {} <Information> BaseDaemon: Stop SignalListener thread
2025.07.02 09:48:06.391476 [ 1 ] {} <Information> Application: Starting ClickHouse Keeper 25.6.2.5 (revision: 54500, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, build id: F98547FCC3F690DCAA9E31DC91EE6FFFE81C69AA), PID 1
2025.07.02 09:48:06.393703 [ 1 ] {} <Information> Application: starting up
2025.07.02 09:48:06.394737 [ 1 ] {} <Information> Application: OS Name = Linux, OS Version = 6.6.87.1-microsoft-standard-WSL2, OS Architecture = x86_64
2025.07.02 09:48:06.395401 [ 1 ] {} <Information> Jemalloc: Value for background_thread set to true (from true)
2025.07.02 09:48:06.395933 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:48:06.400871 [ 1 ] {} <Information> CgroupsReader: Will create cgroup reader from '/sys/fs/cgroup/' (cgroups version: v2)
2025.07.02 09:48:06.401537 [ 1 ] {} <Information> MemoryWorker: Starting background memory thread with period of 50ms, using Cgroups as source
2025.07.02 09:48:06.422846 [ 1 ] {} <Information> Context: Cannot connect to ZooKeeper (or Keeper) before internal Keeper start, will wait for Keeper synchronously
2025.07.02 09:48:06.424418 [ 1 ] {} <Information> KeeperContext: Keeper feature flag FILTERED_LIST: enabled
2025.07.02 09:48:06.425154 [ 1 ] {} <Information> KeeperContext: Keeper feature flag MULTI_READ: enabled
2025.07.02 09:48:06.427781 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CHECK_NOT_EXISTS: disabled
2025.07.02 09:48:06.428819 [ 1 ] {} <Information> KeeperContext: Keeper feature flag CREATE_IF_NOT_EXISTS: disabled
2025.07.02 09:48:06.429353 [ 1 ] {} <Information> KeeperContext: Keeper feature flag REMOVE_RECURSIVE: disabled
2025.07.02 09:48:06.451696 [ 1 ] {} <Warning> KeeperLogStore: No logs exists in /var/lib/clickhouse/coordination/logs. It's Ok if it's the first run of clickhouse-keeper.
2025.07.02 09:48:06.454628 [ 1 ] {} <Information> KeeperLogStore: force_sync enabled
2025.07.02 09:48:06.455505 [ 1 ] {} <Warning> KeeperLogStore: Removing all changelogs
2025.07.02 09:48:06.456070 [ 1 ] {} <Information> KeeperServer: No config in log store and snapshot, probably it's initial run. Will use config from .xml on disk
2025.07.02 09:48:06.462494 [ 29 ] {} <Information> MemoryTracker: Correcting the value of global memory tracker from 113.53 KiB to 28.40 MiB
2025.07.02 09:48:06.485389 [ 1 ] {} <Information> RaftInstance: Raft ASIO listener initiated on :::9234, unsecured
2025.07.02 09:48:06.490478 [ 1 ] {} <Information> RaftInstance: parameters: election timeout range 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, enable randomized snapshot creation NO, log sync stop gap 99999, use new joiner type NO, reserved logs 100000, client timeout 10000, auto forwarding on, API call type async, custom commit quorum size 0, custom election quorum size 0, snapshot receiver included, leadership transfer wait time 0, grace period of lagging state machine 0, snapshot IO: blocking, parallel log appending: on, streaming mode max log gap 0, max bytes 0
2025.07.02 09:48:06.491819 [ 1 ] {} <Information> RaftInstance: new election timeout range: 1000 - 2000
2025.07.02 09:48:06.493317 [ 1 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===
commit index 0
term 0
election timer allowed
catching-up no
log store start 1, end 0
config log idx 0, prev log idx 0
2025.07.02 09:48:06.494369 [ 1 ] {} <Information> RaftInstance: peer 1: DC ID 0, keeper:9234, voting member, 1
my id: 1, voting_member
num peers: 0
2025.07.02 09:48:06.496964 [ 1 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append
2025.07.02 09:48:06.498348 [ 1 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms
2025.07.02 09:48:06.499537 [ 59 ] {} <Information> RaftInstance: bg append_entries thread initiated
2025.07.02 09:48:08.455782 [ 42 ] {} <Warning> RaftInstance: Election timeout, initiate leader election
2025.07.02 09:48:08.457268 [ 42 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 1
2025.07.02 09:48:08.457431 [ 42 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 1, hb dead, pre-vote NOT done
2025.07.02 09:48:08.460515 [ 42 ] {} <Information> RaftInstance: [VOTE INIT] my id 1, my role candidate, term 1, log idx 0, log term 0, priority (target 1 / mine 1)
2025.07.02 09:48:08.460711 [ 42 ] {} <Information> RaftInstance: number of pending commit elements: 0
2025.07.02 09:48:08.460853 [ 42 ] {} <Information> RaftInstance: state machine commit index 0, precommit index 0, last log index 0
2025.07.02 09:48:08.464453 [ 42 ] {} <Information> RaftInstance: [BECOME LEADER] appended new config at 1
2025.07.02 09:48:08.465023 [ 60 ] {} <Information> RaftInstance: config at index 1 is committed, prev config log idx 0
2025.07.02 09:48:08.465225 [ 60 ] {} <Information> RaftInstance: new config log idx 1, prev log idx 0, cur config log idx 0, prev log idx 0
2025.07.02 09:48:08.465406 [ 60 ] {} <Information> RaftInstance: new configuration: log idx 1, prev log idx 0
peer 1, DC ID 0, keeper:9234, voting member, regular member, 1
my id: 1, leader: 1, term: 1
2025.07.02 09:48:08.467063 [ 1 ] {} <Information> Application: Listening for Keeper (tcp): 0.0.0.0:2181
2025.07.02 09:48:08.467527 [ 1 ] {} <Information> Application: Listening for Prometheus: http://0.0.0.0:9363
2025.07.02 09:48:08.486565 [ 1 ] {} <Information> Application: keeper_server.max_memory_usage_soft_limit is set to 6.98 GiB
2025.07.02 09:48:08.496472 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:48:08.496727 [ 1 ] {} <Information> CertificateReloader: One of paths is empty. Cannot apply new configuration for certificates. Fill all paths and try again.
2025.07.02 09:48:08.497443 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Started cgroup current memory usage observer thread
2025.07.02 09:48:08.497599 [ 1 ] {} <Information> Application: Ready for connections.
2025.07.02 09:48:08.497719 [ 67 ] {} <Information> CgroupsMemoryUsageObserver: Memory amount initially available to the process is 7.76 GiB
2025.07.02 09:48:10.470781 [ 30 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:48:10.474679 [ 30 ] {} <Information> KeeperTCPHandler: Received session ID 1
2025.07.02 09:48:10.542134 [ 32 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:48:10.545567 [ 32 ] {} <Information> KeeperTCPHandler: Received session ID 2
2025.07.02 09:48:10.601255 [ 33 ] {} <Information> KeeperTCPHandler: Requesting session ID for the new client
2025.07.02 09:48:10.604933 [ 33 ] {} <Information> KeeperTCPHandler: Received session ID 3
2025.07.02 09:48:20.477177 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9925 ms
2025.07.02 09:48:20.546778 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9943 ms
2025.07.02 09:48:20.606144 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9949 ms
2025.07.02 09:48:30.477159 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10018 ms
2025.07.02 09:48:30.546821 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10016 ms
2025.07.02 09:48:30.606120 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10019 ms
2025.07.02 09:48:39.524595 [ 86 ] {} <Warning> KeeperTCPHandler: Client has not sent any data.
2025.07.02 09:48:40.505293 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10010 ms
2025.07.02 09:48:40.575036 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10013 ms
2025.07.02 09:48:40.634443 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10005 ms
2025.07.02 09:48:50.505687 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9967 ms
2025.07.02 09:48:50.575141 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9970 ms
2025.07.02 09:48:50.634444 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9971 ms
2025.07.02 09:49:00.505293 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10019 ms
2025.07.02 09:49:00.575043 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10020 ms
2025.07.02 09:49:00.634436 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10021 ms
2025.07.02 09:49:10.534236 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 10025 ms
2025.07.02 09:49:10.604024 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 10024 ms
2025.07.02 09:49:10.663369 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 10023 ms
2025.07.02 09:49:20.534249 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 9975 ms
2025.07.02 09:49:20.604158 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 9976 ms
2025.07.02 09:49:20.663358 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 9976 ms
2025.07.02 09:49:25.424858 [ 33 ] {} <Information> KeeperTCPHandler: Receiving request for session 3 took 4767 ms
2025.07.02 09:49:25.426158 [ 32 ] {} <Information> KeeperTCPHandler: Receiving request for session 2 took 4827 ms
2025.07.02 09:49:25.441862 [ 30 ] {} <Information> KeeperTCPHandler: Receiving request for session 1 took 4903 ms
2025.07.02 09:49:26.417374 [ 28 ] {} <Information> Application: Received termination signal (Terminated)
2025.07.02 09:49:26.419021 [ 1 ] {} <Information> CgroupsMemoryUsageObserver: Stopped cgroup current memory usage observer thread
2025.07.02 09:49:26.419325 [ 1 ] {} <Information> Application: Shutting down.
2025.07.02 09:49:26.420828 [ 1 ] {} <Information> Application: Closed all listening sockets.
2025.07.02 09:49:26.421072 [ 1 ] {} <Information> Application: Closed connections to Keeper.
2025.07.02 09:49:26.536992 [ 1 ] {} <Information> RaftInstance: shutting down raft core
2025.07.02 09:49:26.538038 [ 1 ] {} <Information> RaftInstance: sent stop signal to the commit thread.
2025.07.02 09:49:26.538361 [ 1 ] {} <Information> RaftInstance: cancelled all schedulers.
2025.07.02 09:49:26.538670 [ 1 ] {} <Information> RaftInstance: commit thread stopped.
2025.07.02 09:49:26.538980 [ 1 ] {} <Information> RaftInstance: all pending commit elements dropped.
2025.07.02 09:49:26.539165 [ 1 ] {} <Information> RaftInstance: reset all pointers.
2025.07.02 09:49:26.539314 [ 1 ] {} <Information> RaftInstance: joined terminated commit thread.
2025.07.02 09:49:26.539547 [ 59 ] {} <Information> RaftInstance: bg append_entries thread terminated
2025.07.02 09:49:26.539626 [ 1 ] {} <Information> RaftInstance: sent stop signal to background append thread.
2025.07.02 09:49:26.540362 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems
2025.07.02 09:49:26.541509 [ 1 ] {} <Information> RaftInstance: clean up auto-forwarding clients
2025.07.02 09:49:26.543811 [ 1 ] {} <Information> RaftInstance: raft_server shutdown completed.
2025.07.02 09:49:26.544095 [ 1 ] {} <Information> RaftInstance: manually create a snapshot on 64
2025.07.02 09:49:26.544326 [ 1 ] {} <Information> RaftInstance: creating a snapshot for index 64
2025.07.02 09:49:26.544632 [ 1 ] {} <Information> RaftInstance: create snapshot idx 64 log_term 1
2025.07.02 09:49:26.544933 [ 1 ] {} <Information> KeeperStateMachine: Creating a snapshot during shutdown because 'create_snapshot_on_exit' is enabled.
2025.07.02 09:49:26.548915 [ 1 ] {} <Information> RaftInstance: snapshot idx 64 log_term 1 created, compact the log store if needed
2025.07.02 09:49:26.549256 [ 1 ] {} <Information> RaftInstance: create snapshot idx 64 log_term 1 done: 4393 us elapsed
2025.07.02 09:49:26.549711 [ 51 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125, Operation canceled
2025.07.02 09:49:26.550242 [ 55 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10
2025.07.02 09:49:26.550061 [ 57 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:49:26.550137 [ 42 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13
2025.07.02 09:49:26.550158 [ 56 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12
2025.07.02 09:49:26.550174 [ 44 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11
2025.07.02 09:49:26.550050 [ 47 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14
2025.07.02 09:49:26.550300 [ 54 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9
2025.07.02 09:49:26.550377 [ 51 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7
2025.07.02 09:49:26.550606 [ 52 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6
2025.07.02 09:49:26.550716 [ 50 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5
2025.07.02 09:49:26.550425 [ 53 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8
2025.07.02 09:49:26.550816 [ 48 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4
2025.07.02 09:49:26.551195 [ 46 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2
2025.07.02 09:49:26.551296 [ 43 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0
2025.07.02 09:49:26.551234 [ 45 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1
2025.07.02 09:49:26.551804 [ 49 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3
2025.07.02 09:49:26.556137 [ 41 ] {} <Information> KeeperLogStore: Raft server is not set in LogStore.
2025.07.02 09:49:26.557400 [ 1 ] {} <Information> KeeperSnapshotManagerS3: KeeperSnapshotManagerS3 shut down
2025.07.02 09:49:26.558248 [ 1 ] {} <Information> KeeperLogStore: Changelog is shut down
2025.07.02 09:49:26.562776 [ 1 ] {} <Information> Application: Waiting for background threads
2025.07.02 09:49:26.564508 [ 1 ] {} <Information> Application: Background threads finished in 1 ms
2025.07.02 09:49:26.567943 [ 1 ] {} <Information> Application: shutting down
2025.07.02 09:49:26.568339 [ 28 ] {} <Information> BaseDaemon: Stop SignalListener thread
